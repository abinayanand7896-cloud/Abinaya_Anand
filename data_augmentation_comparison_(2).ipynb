{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abinayanand7896-cloud/Abinaya_Anand/blob/main/data_augmentation_comparison_(2).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1",
      "metadata": {
        "id": "1"
      },
      "source": [
        "# Data Augmentation Techniques Comparison Pipeline\n",
        "\n",
        "This notebook compares SMOTE, ADASYN, and CTGAN data augmentation techniques on imbalanced classification datasets.\n",
        "\n",
        "**Datasets:**\n",
        "1. Breast Cancer (sklearn)\n",
        "2. Credit Card Fraud (uploaded CSV)\n",
        "\n",
        "**Imbalance Ratios:** 1:10, 1:100, 1:1000\n",
        "\n",
        "**Classifiers:** Logistic Regression, Random Forest, XGBoost\n",
        "\n",
        "**Augmentation Variations:** Default parameters vs Tuned parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2",
        "outputId": "360d4e13-7ca3-4bb9-d6f5-a52d2bcb5c4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.12/dist-packages (0.14.1)\n",
            "Collecting ctgan\n",
            "  Downloading ctgan-0.12.1-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting sdv\n",
            "  Downloading sdv-1.34.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.12/dist-packages (3.2.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.12/dist-packages (3.1.5)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: sklearn-compat<0.2,>=0.1.5 in /usr/local/lib/python3.12/dist-packages (from imbalanced-learn) (0.1.5)\n",
            "Requirement already satisfied: torch>=2.3.0 in /usr/local/lib/python3.12/dist-packages (from ctgan) (2.10.0+cu128)\n",
            "Requirement already satisfied: tqdm>=4.29 in /usr/local/lib/python3.12/dist-packages (from ctgan) (4.67.3)\n",
            "Collecting rdt>=1.14.0 (from ctgan)\n",
            "  Downloading rdt-1.20.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting boto3<2.0.0,>=1.28 (from sdv)\n",
            "  Downloading boto3-1.42.54-py3-none-any.whl.metadata (6.7 kB)\n",
            "Collecting botocore<2.0.0,>=1.31 (from sdv)\n",
            "  Downloading botocore-1.42.54-py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: cloudpickle>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from sdv) (3.1.2)\n",
            "Requirement already satisfied: graphviz>=0.13.2 in /usr/local/lib/python3.12/dist-packages (from sdv) (0.21)\n",
            "Collecting copulas>=0.12.1 (from sdv)\n",
            "  Downloading copulas-0.14.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting deepecho>=0.7.0 (from sdv)\n",
            "  Downloading deepecho-0.8.1-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting sdmetrics>=0.21.0 (from sdv)\n",
            "  Downloading sdmetrics-0.27.1-py3-none-any.whl.metadata (10.0 kB)\n",
            "Requirement already satisfied: platformdirs>=4.0 in /usr/local/lib/python3.12/dist-packages (from sdv) (4.9.2)\n",
            "Requirement already satisfied: pyyaml>=6.0.1 in /usr/local/lib/python3.12/dist-packages (from sdv) (6.0.3)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.12/dist-packages (from xgboost) (2.27.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.3)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.12/dist-packages (from openpyxl) (2.0.0)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3<2.0.0,>=1.28->sdv)\n",
            "  Downloading jmespath-1.1.0-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting s3transfer<0.17.0,>=0.16.0 (from boto3<2.0.0,>=1.28->sdv)\n",
            "  Downloading s3transfer-0.16.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.12/dist-packages (from botocore<2.0.0,>=1.31->sdv) (2.5.0)\n",
            "Requirement already satisfied: plotly>=5.10.0 in /usr/local/lib/python3.12/dist-packages (from copulas>=0.12.1->sdv) (5.24.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Collecting Faker!=37.11.0,>=17 (from rdt>=1.14.0->ctgan)\n",
            "  Downloading faker-40.4.0-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->ctgan) (3.24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->ctgan) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->ctgan) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->ctgan) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->ctgan) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->ctgan) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->ctgan) (2025.3.0)\n",
            "Requirement already satisfied: cuda-bindings==12.9.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->ctgan) (12.9.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->ctgan) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->ctgan) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->ctgan) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->ctgan) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->ctgan) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->ctgan) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->ctgan) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->ctgan) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->ctgan) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->ctgan) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.4.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->ctgan) (3.4.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->ctgan) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->ctgan) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->ctgan) (1.13.1.3)\n",
            "Requirement already satisfied: triton==3.6.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->ctgan) (3.6.0)\n",
            "Requirement already satisfied: cuda-pathfinder~=1.1 in /usr/local/lib/python3.12/dist-packages (from cuda-bindings==12.9.4->torch>=2.3.0->ctgan) (1.3.4)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly>=5.10.0->copulas>=0.12.1->sdv) (9.1.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from plotly>=5.10.0->copulas>=0.12.1->sdv) (26.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.3.0->ctgan) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.3.0->ctgan) (3.0.3)\n",
            "Downloading ctgan-0.12.1-py3-none-any.whl (25 kB)\n",
            "Downloading sdv-1.34.1-py3-none-any.whl (200 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.2/200.2 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading boto3-1.42.54-py3-none-any.whl (140 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.6/140.6 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading botocore-1.42.54-py3-none-any.whl (14.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m113.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading copulas-0.14.1-py3-none-any.whl (52 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.7/52.7 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading deepecho-0.8.1-py3-none-any.whl (28 kB)\n",
            "Downloading rdt-1.20.0-py3-none-any.whl (74 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.5/74.5 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sdmetrics-0.27.1-py3-none-any.whl (201 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.5/201.5 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading faker-40.4.0-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m59.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jmespath-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading s3transfer-0.16.0-py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jmespath, Faker, botocore, s3transfer, rdt, copulas, sdmetrics, deepecho, ctgan, boto3, sdv\n",
            "Successfully installed Faker-40.4.0 boto3-1.42.54 botocore-1.42.54 copulas-0.14.1 ctgan-0.12.1 deepecho-0.8.1 jmespath-1.1.0 rdt-1.20.0 s3transfer-0.16.0 sdmetrics-0.27.1 sdv-1.34.1\n"
          ]
        }
      ],
      "source": [
        "!pip install scikit-learn imbalanced-learn ctgan sdv xgboost pandas numpy openpyxl"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3",
      "metadata": {
        "id": "3"
      },
      "source": [
        "## Import Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "4",
      "metadata": {
        "id": "4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import json\n",
        "import warnings\n",
        "from collections import defaultdict\n",
        "import zipfile\n",
        "\n",
        "# ML Libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Augmentation techniques\n",
        "from imblearn.over_sampling import SMOTE, ADASYN\n",
        "from ctgan import CTGAN\n",
        "\n",
        "# Metrics\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score\n",
        "\n",
        "# Google Colab utilities\n",
        "from google.colab import files\n",
        "from google.colab import drive\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5",
      "metadata": {
        "id": "5"
      },
      "source": [
        "## Data Loading and Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6",
        "outputId": "fdfc2b4a-9e54-457f-fbac-eb479ae15176"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cancer Diagnosis shape: (4024, 16)\n",
            "Cancer target: 'Status'\n",
            "Class distribution:\n",
            "Status\n",
            "Alive    3408\n",
            "Dead      616\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "cancer_df = pd.read_csv('/content/Breast_Cancer.csv')\n",
        "print(f\"Cancer Diagnosis shape: {cancer_df.shape}\")\n",
        "\n",
        "cancer_target = 'Status' if 'Status' in cancer_df.columns else cancer_df.columns[-1]\n",
        "print(f\"Cancer target: '{cancer_target}'\")\n",
        "print(f\"Class distribution:\\n{cancer_df[cancer_target].value_counts()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "3YXfQrhh2_tq",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3YXfQrhh2_tq",
        "outputId": "a83b741e-139c-4336-d13e-5a00a967df43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fraud Detection shape: (15936, 31)\n",
            "Fraud target: 'Class'\n",
            "Class distribution:\n",
            "Class\n",
            "0.0    15862\n",
            "1.0       73\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "fraud_df = pd.read_csv('/content/creditcard.csv')\n",
        "print(f\"Fraud Detection shape: {fraud_df.shape}\")\n",
        "\n",
        "# Identify target column (last column or 'Class')\n",
        "fraud_target = 'Class' if 'Class' in fraud_df.columns else fraud_df.columns[-1]\n",
        "print(f\"Fraud target: '{fraud_target}'\")\n",
        "print(f\"Class distribution:\\n{fraud_df[fraud_target].value_counts()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8",
      "metadata": {
        "id": "8"
      },
      "source": [
        "## Step 1: Create Imbalanced Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9",
        "outputId": "37c41b2e-dd9c-4457-d307-edb9b46d87a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "CREATING BREAST CANCER IMBALANCED DATASETS\n",
            "==================================================\n",
            "\n",
            "Creating Breast Cancer 1:10 with 1:10 ratio\n",
            "Minority class (Dead): 616 samples\n",
            "Target majority samples: 6160\n",
            "Bootstrapped majority class to 6160 samples (repeated 2 times)\n",
            "Final dataset shape: (6776, 16)\n",
            "Final class distribution: {'Alive': 6160, 'Dead': 616}\n",
            "Saved: imbalanced_datasets/breast_cancer_1_10.csv\n",
            "\n",
            "Creating Breast Cancer 1:100 with 1:100 ratio\n",
            "Minority class (Dead): 616 samples\n",
            "Target majority samples: 61600\n",
            "Bootstrapped majority class to 61600 samples (repeated 19 times)\n",
            "Final dataset shape: (62216, 16)\n",
            "Final class distribution: {'Alive': 61600, 'Dead': 616}\n",
            "Saved: imbalanced_datasets/breast_cancer_1_100.csv\n",
            "\n",
            "==================================================\n",
            "CREATING CREDIT CARD IMBALANCED DATASETS\n",
            "==================================================\n",
            "\n",
            "Creating Credit Card 1:10 with 1:10 ratio\n",
            "Minority class (1.0): 73 samples\n",
            "Target majority samples: 730\n",
            "Subsampled majority class to 730 samples\n",
            "Final dataset shape: (803, 31)\n",
            "Final class distribution: {0.0: 730, 1.0: 73}\n",
            "Saved: imbalanced_datasets/creditcard_1_10.csv\n",
            "\n",
            "Creating Credit Card 1:100 with 1:100 ratio\n",
            "Minority class (1.0): 73 samples\n",
            "Target majority samples: 7300\n",
            "Subsampled majority class to 7300 samples\n",
            "Final dataset shape: (7373, 31)\n",
            "Final class distribution: {0.0: 7300, 1.0: 73}\n",
            "Saved: imbalanced_datasets/creditcard_1_100.csv\n"
          ]
        }
      ],
      "source": [
        "def create_imbalanced_dataset(df, target_col, ratio, dataset_name):\n",
        "    \"\"\"\n",
        "    Create imbalanced dataset with specified minority:majority ratio\n",
        "    \"\"\"\n",
        "    minority_class = df[target_col].value_counts().idxmin()\n",
        "    majority_class = df[target_col].value_counts().idxmax()\n",
        "\n",
        "    minority_samples = df[df[target_col] == minority_class]\n",
        "    majority_samples = df[df[target_col] == majority_class]\n",
        "\n",
        "    n_minority = len(minority_samples)\n",
        "    n_majority_target = n_minority * ratio\n",
        "\n",
        "    print(f\"\\nCreating {dataset_name} with 1:{ratio} ratio\")\n",
        "    print(f\"Minority class ({minority_class}): {n_minority} samples\")\n",
        "    print(f\"Target majority samples: {n_majority_target}\")\n",
        "\n",
        "    # Handle different scenarios based on available majority samples\n",
        "    if len(majority_samples) >= n_majority_target:\n",
        "        # Subsample majority class\n",
        "        majority_selected = majority_samples.sample(n=n_majority_target, random_state=42)\n",
        "        print(f\"Subsampled majority class to {len(majority_selected)} samples\")\n",
        "    else:\n",
        "        # Bootstrap/duplicate majority class (for breast cancer with high ratios)\n",
        "        n_repeats = int(np.ceil(n_majority_target / len(majority_samples)))\n",
        "        majority_repeated = pd.concat([majority_samples] * n_repeats, ignore_index=True)\n",
        "        majority_selected = majority_repeated.sample(n=n_majority_target, random_state=42)\n",
        "        print(f\"Bootstrapped majority class to {len(majority_selected)} samples (repeated {n_repeats} times)\")\n",
        "\n",
        "    # Combine minority and majority samples\n",
        "    imbalanced_df = pd.concat([minority_samples, majority_selected], ignore_index=True)\n",
        "    imbalanced_df = imbalanced_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "    print(f\"Final dataset shape: {imbalanced_df.shape}\")\n",
        "    print(f\"Final class distribution: {imbalanced_df[target_col].value_counts().to_dict()}\")\n",
        "\n",
        "    return imbalanced_df\n",
        "\n",
        "# Create imbalanced datasets\n",
        "ratios = [10, 100]\n",
        "\n",
        "# Create directory if it doesn't exist\n",
        "os.makedirs('imbalanced_datasets', exist_ok=True)\n",
        "\n",
        "print(\"=\" * 50)\n",
        "print(\"CREATING BREAST CANCER IMBALANCED DATASETS\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "for ratio in ratios:\n",
        "    imbalanced_bc = create_imbalanced_dataset(cancer_df, 'Status', ratio, f\"Breast Cancer 1:{ratio}\")\n",
        "    filename = f\"imbalanced_datasets/breast_cancer_1_{ratio}.csv\"\n",
        "    imbalanced_bc.to_csv(filename, index=False)\n",
        "    print(f\"Saved: {filename}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"CREATING CREDIT CARD IMBALANCED DATASETS\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "for ratio in ratios:\n",
        "    imbalanced_cc = create_imbalanced_dataset(fraud_df, 'Class', ratio, f\"Credit Card 1:{ratio}\")\n",
        "    filename = f\"imbalanced_datasets/creditcard_1_{ratio}.csv\"\n",
        "    imbalanced_cc.to_csv(filename, index=False)\n",
        "    print(f\"Saved: {filename}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10",
      "metadata": {
        "id": "10"
      },
      "source": [
        "## Step 2: Define Fixed Classifiers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "11",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11",
        "outputId": "e2de84ea-3b70-4d6f-85b8-6bec2806a06b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fixed classifiers defined:\n",
            "- LogisticRegression: LogisticRegression(max_iter=1000, random_state=42)\n",
            "- RandomForest: RandomForestClassifier(random_state=42)\n",
            "- XGBoost: XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
            "              colsample_bylevel=None, colsample_bynode=None,\n",
            "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
            "              enable_categorical=False, eval_metric='logloss',\n",
            "              feature_types=None, feature_weights=None, gamma=None,\n",
            "              grow_policy=None, importance_type=None,\n",
            "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
            "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
            "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
            "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
            "              multi_strategy=None, n_estimators=100, n_jobs=None,\n",
            "              num_parallel_tree=None, ...)\n"
          ]
        }
      ],
      "source": [
        "def get_classifiers():\n",
        "    \"\"\"\n",
        "    Return fixed classifiers with specified parameters\n",
        "    \"\"\"\n",
        "    return {\n",
        "        'LogisticRegression': LogisticRegression(max_iter=1000, random_state=42),\n",
        "        'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "        'XGBoost': XGBClassifier(n_estimators=100, eval_metric='logloss',\n",
        "                                random_state=42, use_label_encoder=False)\n",
        "    }\n",
        "\n",
        "print(\"Fixed classifiers defined:\")\n",
        "for name, clf in get_classifiers().items():\n",
        "    print(f\"- {name}: {clf}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12",
      "metadata": {
        "id": "12"
      },
      "source": [
        "## Step 3: Define Augmentation Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "13",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13",
        "outputId": "66b8e98b-b91a-4e14-d6ed-cc1ab4a44b71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Augmentation configurations:\n",
            "\n",
            "V1_Default:\n",
            "  SMOTE: {'k_neighbors': 5, 'sampling_strategy': 'auto', 'random_state': 42}\n",
            "  ADASYN: {'n_neighbors': 5, 'sampling_strategy': 'auto', 'random_state': 42}\n",
            "  CTGAN: {'epochs': 10, 'batch_size': 500}\n",
            "\n",
            "V2_Tuned:\n",
            "  SMOTE: {'k_neighbors': 3, 'sampling_strategy': 0.5, 'random_state': 42}\n",
            "  ADASYN: {'n_neighbors': 10, 'sampling_strategy': 0.5, 'random_state': 42}\n",
            "  CTGAN: {'epochs': 20, 'batch_size': 100}\n"
          ]
        }
      ],
      "source": [
        "def get_augmentation_configs():\n",
        "    \"\"\"\n",
        "    Return augmentation configurations for V1 (Default) and V2 (Tuned)\n",
        "    \"\"\"\n",
        "    return {\n",
        "        'V1_Default': {\n",
        "            'SMOTE': {'k_neighbors': 5, 'sampling_strategy': 'auto', 'random_state': 42},\n",
        "            'ADASYN': {'n_neighbors': 5, 'sampling_strategy': 'auto', 'random_state': 42},\n",
        "            'CTGAN': {'epochs': 10, 'batch_size': 500}\n",
        "        },\n",
        "        'V2_Tuned': {\n",
        "            'SMOTE': {'k_neighbors': 3, 'sampling_strategy': 0.5, 'random_state': 42},\n",
        "            'ADASYN': {'n_neighbors': 10, 'sampling_strategy': 0.5, 'random_state': 42},\n",
        "            'CTGAN': {'epochs': 20, 'batch_size': 100}\n",
        "        }\n",
        "    }\n",
        "\n",
        "configs = get_augmentation_configs()\n",
        "print(\"Augmentation configurations:\")\n",
        "for variation, methods in configs.items():\n",
        "    print(f\"\\n{variation}:\")\n",
        "    for method, params in methods.items():\n",
        "        print(f\"  {method}: {params}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14",
      "metadata": {
        "id": "14"
      },
      "source": [
        "## Step 4: Training and Evaluation Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "15",
      "metadata": {
        "id": "15"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(clf, X_test, y_test):\n",
        "    \"\"\"\n",
        "    Evaluate model and return metrics for minority class (always class 1)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        y_pred = clf.predict(X_test)\n",
        "        y_pred_proba = clf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "        # Verify minority class is 1\n",
        "        class_counts = np.bincount(y_test)\n",
        "        minority_class = np.argmin(class_counts)\n",
        "        if minority_class != 1:\n",
        "            print(f\"Warning: Expected minority class to be 1, but found {minority_class}\")\n",
        "\n",
        "        metrics = {\n",
        "            'F1': round(f1_score(y_test, y_pred, pos_label=1), 4),\n",
        "            'Precision': round(precision_score(y_test, y_pred, pos_label=1), 4),\n",
        "            'Recall': round(recall_score(y_test, y_pred, pos_label=1), 4),\n",
        "            'AUC-ROC': round(roc_auc_score(y_test, y_pred_proba), 4)\n",
        "        }\n",
        "        return metrics\n",
        "    except Exception as e:\n",
        "        print(f\"Error in evaluation: {str(e)}\")\n",
        "        return {'F1': np.nan, 'Precision': np.nan, 'Recall': np.nan, 'AUC-ROC': np.nan}\n",
        "\n",
        "def run_experiment(df, target_col, dataset_name, ratio):\n",
        "    \"\"\"\n",
        "    Run complete experiment for one dataset and ratio\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"RUNNING EXPERIMENT: {dataset_name} - Ratio 1:{ratio}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # Prepare features and target\n",
        "    X = df.drop(target_col, axis=1).values\n",
        "    y = df[target_col].values\n",
        "\n",
        "    # Verify minority class is 1\n",
        "    class_counts = np.bincount(y)\n",
        "    minority_class = np.argmin(class_counts)\n",
        "    print(f\"Dataset class distribution: {dict(enumerate(class_counts))}\")\n",
        "    print(f\"Minority class: {minority_class} ({class_counts[minority_class]} samples)\")\n",
        "    print(f\"Majority class: {1-minority_class} ({class_counts[1-minority_class]} samples)\")\n",
        "\n",
        "    if minority_class != 1:\n",
        "        print(f\"ERROR: Expected minority class to be 1, but found {minority_class}\")\n",
        "        print(\"Please check dataset preprocessing!\")\n",
        "\n",
        "    # Train-test split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, stratify=y, random_state=42\n",
        "    )\n",
        "\n",
        "    # Standardize features\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    print(f\"Train shape: {X_train_scaled.shape}, Test shape: {X_test_scaled.shape}\")\n",
        "    print(f\"Train class distribution: {np.bincount(y_train)}\")\n",
        "    print(f\"Test class distribution: {np.bincount(y_test)}\")\n",
        "\n",
        "    # Initialize results storage\n",
        "    results = defaultdict(dict)\n",
        "    classifiers = get_classifiers()\n",
        "    augmentation_configs = get_augmentation_configs()\n",
        "\n",
        "    # Baseline (no augmentation) - compute once, use for both variations\n",
        "    print(\"\\n--- Running Baseline (No Augmentation) ---\")\n",
        "    baseline_results = {}\n",
        "    for clf_name, clf in classifiers.items():\n",
        "        print(f\"Training {clf_name}...\")\n",
        "        clf.fit(X_train_scaled, y_train)\n",
        "        metrics = evaluate_model(clf, X_test_scaled, y_test)\n",
        "        baseline_results[clf_name] = metrics\n",
        "        print(f\"  {clf_name} - F1: {metrics['F1']}, Precision: {metrics['Precision']}, Recall: {metrics['Recall']}, AUC: {metrics['AUC-ROC']}\")\n",
        "\n",
        "    # Store baseline for both variations\n",
        "    results['V1_Default']['Baseline'] = baseline_results.copy()\n",
        "    results['V2_Tuned']['Baseline'] = baseline_results.copy()\n",
        "\n",
        "    # Run augmentation methods for each variation\n",
        "    for variation_name, aug_config in augmentation_configs.items():\n",
        "        print(f\"\\n--- Running {variation_name} ---\")\n",
        "\n",
        "        for aug_method, aug_params in aug_config.items():\n",
        "            print(f\"\\nApplying {aug_method} augmentation...\")\n",
        "\n",
        "            # Apply augmentation\n",
        "            X_train_aug, y_train_aug = apply_augmentation(\n",
        "                X_train_scaled, y_train, aug_method, aug_params\n",
        "            )\n",
        "\n",
        "            print(f\"  Augmented training shape: {X_train_aug.shape}\")\n",
        "            print(f\"  Augmented class distribution: {np.bincount(y_train_aug)}\")\n",
        "\n",
        "            # Train classifiers on augmented data\n",
        "            method_results = {}\n",
        "            for clf_name, clf in classifiers.items():\n",
        "                print(f\"  Training {clf_name}...\")\n",
        "                try:\n",
        "                    clf.fit(X_train_aug, y_train_aug)\n",
        "                    metrics = evaluate_model(clf, X_test_scaled, y_test)\n",
        "                    method_results[clf_name] = metrics\n",
        "                    print(f\"    {clf_name} - F1: {metrics['F1']}, Precision: {metrics['Precision']}, Recall: {metrics['Recall']}, AUC: {metrics['AUC-ROC']}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"    Error training {clf_name}: {str(e)}\")\n",
        "                    method_results[clf_name] = {'F1': np.nan, 'Precision': np.nan, 'Recall': np.nan, 'AUC-ROC': np.nan}\n",
        "\n",
        "            results[variation_name][aug_method] = method_results\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "27cd7367",
      "metadata": {
        "id": "27cd7367"
      },
      "outputs": [],
      "source": [
        "def apply_augmentation(X_train, y_train, method, params):\n",
        "    \"\"\"\n",
        "    Apply specified augmentation method to training data\n",
        "    \"\"\"\n",
        "    if method == 'SMOTE':\n",
        "        smote = SMOTE(**params)\n",
        "        X_res, y_res = smote.fit_resample(X_train, y_train)\n",
        "    elif method == 'ADASYN':\n",
        "        adasyn = ADASYN(**params)\n",
        "        X_res, y_res = adasyn.fit_resample(X_train, y_train)\n",
        "    elif method == 'CTGAN':\n",
        "        # CTGAN requires dataframe and specific data types\n",
        "        # Convert numpy arrays back to DataFrame for CTGAN\n",
        "        # Assign string column names to prevent TypeError in rdt library\n",
        "        feature_cols = [f'feature_{i}' for i in range(X_train.shape[1])]\n",
        "        X_train_df = pd.DataFrame(X_train, columns=feature_cols)\n",
        "        y_train_df = pd.DataFrame(y_train, columns=['target'])\n",
        "        # Combine X and y for CTGAN training\n",
        "        data_for_ctgan = pd.concat([X_train_df, y_train_df], axis=1)\n",
        "\n",
        "        # Define categorical features for CTGAN (only target in this case)\n",
        "        categorical_features = ['target']\n",
        "        discrete_columns = [col for col in categorical_features if col in data_for_ctgan.columns]\n",
        "\n",
        "        # Ensure target column is treated as categorical by CTGAN\n",
        "        ctgan_model = CTGAN(epochs=params['epochs'], batch_size=params['batch_size'])\n",
        "        ctgan_model.fit(data_for_ctgan, discrete_columns=discrete_columns)\n",
        "\n",
        "        # Generate samples for the minority class\n",
        "        # Determine how many samples to generate to balance the dataset\n",
        "        minority_class = y_train_df['target'].value_counts().idxmin()\n",
        "        majority_class_count = y_train_df['target'].value_counts().max()\n",
        "        minority_class_count = y_train_df['target'].value_counts().min()\n",
        "\n",
        "        num_samples_to_generate = majority_class_count - minority_class_count\n",
        "\n",
        "        # Generate synthetic data with specific conditions if possible (e.g., for minority class)\n",
        "        # Note: CTGAN's generate method doesn't directly support generating only a specific class easily.\n",
        "        # A common approach is to oversample the original minority class within the CTGAN training data\n",
        "        # or filter generated data. For simplicity here, we'll generate and then filter/select.\n",
        "        # A more robust solution might involve conditional GANs or generating more data than needed and filtering.\n",
        "        synthetic_data = ctgan_model.sample(num_samples_to_generate)\n",
        "\n",
        "        # Filter synthetic data to primarily get minority class samples\n",
        "        # This is an approximation; ideally, CTGAN would be trained with a focus on minority class\n",
        "        synthetic_minority_samples = synthetic_data[synthetic_data['target'] == minority_class]\n",
        "\n",
        "        # If not enough minority samples generated, supplement with random samples from all generated\n",
        "        if len(synthetic_minority_samples) < num_samples_to_generate:\n",
        "            remaining_needed = num_samples_to_generate - len(synthetic_minority_samples)\n",
        "            # Take random samples from the rest of synthetic data until target count is met\n",
        "            # This might not be ideal as it could include majority class samples\n",
        "            synthetic_minority_samples = pd.concat([\n",
        "                synthetic_minority_samples,\n",
        "                synthetic_data[synthetic_data['target'] != minority_class].sample(n=remaining_needed, replace=True, random_state=42)\n",
        "            ]).reset_index(drop=True)\n",
        "\n",
        "        # Combine original data with synthetic minority samples\n",
        "        X_res_df = pd.concat([X_train_df, synthetic_minority_samples.drop(columns=['target'])], ignore_index=True)\n",
        "        y_res_df = pd.concat([y_train_df, synthetic_minority_samples[['target']]], ignore_index=True)\n",
        "\n",
        "        X_res = X_res_df.values\n",
        "        y_res = y_res_df['target'].values\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown augmentation method: {method}\")\n",
        "\n",
        "    return X_res, y_res"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16",
      "metadata": {
        "id": "16"
      },
      "source": [
        "## Step 5: Execute Experiments and Generate Comparison Tables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "17",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17",
        "outputId": "b1f592b0-9dd6-4f9b-d244-10892ececc97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "RUNNING EXPERIMENT: Breast Cancer - Ratio 1:10\n",
            "============================================================\n",
            "Dataset class distribution: {0: np.int64(6160), 1: np.int64(616)}\n",
            "Minority class: 1 (616 samples)\n",
            "Majority class: 0 (6160 samples)\n",
            "Train shape: (5420, 29), Test shape: (1356, 29)\n",
            "Train class distribution: [4927  493]\n",
            "Test class distribution: [1233  123]\n",
            "\n",
            "--- Running Baseline (No Augmentation) ---\n",
            "Training LogisticRegression...\n",
            "  LogisticRegression - F1: 0.5169, Precision: 0.8364, Recall: 0.374, AUC: 0.8684\n",
            "Training RandomForest...\n",
            "  RandomForest - F1: 0.6023, Precision: 1.0, Recall: 0.4309, AUC: 0.9258\n",
            "Training XGBoost...\n",
            "  XGBoost - F1: 0.6489, Precision: 0.9385, Recall: 0.4959, AUC: 0.8642\n",
            "\n",
            "--- Running V1_Default ---\n",
            "\n",
            "Applying SMOTE augmentation...\n",
            "  Augmented training shape: (9854, 29)\n",
            "  Augmented class distribution: [4927 4927]\n",
            "  Training LogisticRegression...\n",
            "    LogisticRegression - F1: 0.4099, Precision: 0.2835, Recall: 0.7398, AUC: 0.8603\n",
            "  Training RandomForest...\n",
            "    RandomForest - F1: 0.6893, Precision: 0.8554, Recall: 0.5772, AUC: 0.9296\n",
            "  Training XGBoost...\n",
            "    XGBoost - F1: 0.6598, Precision: 0.9014, Recall: 0.5203, AUC: 0.8613\n",
            "\n",
            "Applying ADASYN augmentation...\n",
            "  Augmented training shape: (9803, 29)\n",
            "  Augmented class distribution: [4927 4876]\n",
            "  Training LogisticRegression...\n",
            "    LogisticRegression - F1: 0.3874, Precision: 0.2614, Recall: 0.748, AUC: 0.8578\n",
            "  Training RandomForest...\n",
            "    RandomForest - F1: 0.6699, Precision: 0.8313, Recall: 0.561, AUC: 0.9279\n",
            "  Training XGBoost...\n",
            "    XGBoost - F1: 0.6667, Precision: 0.9028, Recall: 0.5285, AUC: 0.8637\n",
            "\n",
            "Applying CTGAN augmentation...\n",
            "  Augmented training shape: (9854, 29)\n",
            "  Augmented class distribution: [7560 2294]\n",
            "  Training LogisticRegression...\n",
            "    LogisticRegression - F1: 0.2657, Precision: 0.95, Recall: 0.1545, AUC: 0.8262\n",
            "  Training RandomForest...\n",
            "    RandomForest - F1: 0.618, Precision: 1.0, Recall: 0.4472, AUC: 0.926\n",
            "  Training XGBoost...\n",
            "    XGBoost - F1: 0.6096, Precision: 0.8906, Recall: 0.4634, AUC: 0.8777\n",
            "\n",
            "--- Running V2_Tuned ---\n",
            "\n",
            "Applying SMOTE augmentation...\n",
            "  Augmented training shape: (7390, 29)\n",
            "  Augmented class distribution: [4927 2463]\n",
            "  Training LogisticRegression...\n",
            "    LogisticRegression - F1: 0.5143, Precision: 0.4219, Recall: 0.6585, AUC: 0.8613\n",
            "  Training RandomForest...\n",
            "    RandomForest - F1: 0.6599, Precision: 0.8784, Recall: 0.5285, AUC: 0.9289\n",
            "  Training XGBoost...\n",
            "    XGBoost - F1: 0.6392, Precision: 0.8732, Recall: 0.5041, AUC: 0.8556\n",
            "\n",
            "Applying ADASYN augmentation...\n",
            "  Augmented training shape: (7465, 29)\n",
            "  Augmented class distribution: [4927 2538]\n",
            "  Training LogisticRegression...\n",
            "    LogisticRegression - F1: 0.4852, Precision: 0.3814, Recall: 0.6667, AUC: 0.8594\n",
            "  Training RandomForest...\n",
            "    RandomForest - F1: 0.6701, Precision: 0.8919, Recall: 0.5366, AUC: 0.9263\n",
            "  Training XGBoost...\n",
            "    XGBoost - F1: 0.6421, Precision: 0.9104, Recall: 0.4959, AUC: 0.8673\n",
            "\n",
            "Applying CTGAN augmentation...\n",
            "  Augmented training shape: (9854, 29)\n",
            "  Augmented class distribution: [7490 2364]\n",
            "  Training LogisticRegression...\n",
            "    LogisticRegression - F1: 0.4309, Precision: 0.6724, Recall: 0.3171, AUC: 0.8315\n",
            "  Training RandomForest...\n",
            "    RandomForest - F1: 0.6333, Precision: 1.0, Recall: 0.4634, AUC: 0.9165\n",
            "  Training XGBoost...\n",
            "    XGBoost - F1: 0.6304, Precision: 0.9508, Recall: 0.4715, AUC: 0.8735\n",
            "\n",
            "\n",
            "================================================================================\n",
            "COMPARISON TABLE: Breast Cancer - Ratio 1:10\n",
            "================================================================================\n",
            "Variation                   V1_Default                           V2_Tuned                          \n",
            "Metric                              F1 Precision  Recall AUC-ROC       F1 Precision  Recall AUC-ROC\n",
            "Method   Classifier                                                                                \n",
            "Baseline LogisticRegression     0.5169    0.8364   0.374  0.8684   0.5169    0.8364   0.374  0.8684\n",
            "         RandomForest           0.6023       1.0  0.4309  0.9258   0.6023       1.0  0.4309  0.9258\n",
            "         XGBoost                0.6489    0.9385  0.4959  0.8642   0.6489    0.9385  0.4959  0.8642\n",
            "SMOTE    LogisticRegression     0.4099    0.2835  0.7398  0.8603   0.5143    0.4219  0.6585  0.8613\n",
            "         RandomForest           0.6893    0.8554  0.5772  0.9296   0.6599    0.8784  0.5285  0.9289\n",
            "         XGBoost                0.6598    0.9014  0.5203  0.8613   0.6392    0.8732  0.5041  0.8556\n",
            "ADASYN   LogisticRegression     0.3874    0.2614   0.748  0.8578   0.4852    0.3814  0.6667  0.8594\n",
            "         RandomForest           0.6699    0.8313   0.561  0.9279   0.6701    0.8919  0.5366  0.9263\n",
            "         XGBoost                0.6667    0.9028  0.5285  0.8637   0.6421    0.9104  0.4959  0.8673\n",
            "CTGAN    LogisticRegression     0.2657      0.95  0.1545  0.8262   0.4309    0.6724  0.3171  0.8315\n",
            "         RandomForest            0.618       1.0  0.4472   0.926   0.6333       1.0  0.4634  0.9165\n",
            "         XGBoost                0.6096    0.8906  0.4634  0.8777   0.6304    0.9508  0.4715  0.8735\n",
            "================================================================================\n",
            "\n",
            "============================================================\n",
            "RUNNING EXPERIMENT: Breast Cancer - Ratio 1:100\n",
            "============================================================\n",
            "Dataset class distribution: {0: np.int64(61600), 1: np.int64(616)}\n",
            "Minority class: 1 (616 samples)\n",
            "Majority class: 0 (61600 samples)\n",
            "Train shape: (49772, 29), Test shape: (12444, 29)\n",
            "Train class distribution: [49279   493]\n",
            "Test class distribution: [12321   123]\n",
            "\n",
            "--- Running Baseline (No Augmentation) ---\n",
            "Training LogisticRegression...\n",
            "  LogisticRegression - F1: 0.2553, Precision: 1.0, Recall: 0.1463, AUC: 0.8604\n",
            "Training RandomForest...\n",
            "  RandomForest - F1: 0.6023, Precision: 1.0, Recall: 0.4309, AUC: 0.9837\n",
            "Training XGBoost...\n",
            "  XGBoost - F1: 0.6023, Precision: 1.0, Recall: 0.4309, AUC: 0.8688\n",
            "\n",
            "--- Running V1_Default ---\n",
            "\n",
            "Applying SMOTE augmentation...\n",
            "  Augmented training shape: (98558, 29)\n",
            "  Augmented class distribution: [49279 49279]\n",
            "  Training LogisticRegression...\n",
            "    LogisticRegression - F1: 0.0729, Precision: 0.0383, Recall: 0.7398, AUC: 0.8587\n",
            "  Training RandomForest...\n",
            "    RandomForest - F1: 0.7513, Precision: 1.0, Recall: 0.6016, AUC: 1.0\n",
            "  Training XGBoost...\n",
            "    XGBoost - F1: 0.6484, Precision: 1.0, Recall: 0.4797, AUC: 0.8616\n",
            "\n",
            "Applying ADASYN augmentation...\n",
            "  Augmented training shape: (98693, 29)\n",
            "  Augmented class distribution: [49279 49414]\n",
            "  Training LogisticRegression...\n",
            "    LogisticRegression - F1: 0.0685, Precision: 0.0359, Recall: 0.748, AUC: 0.8588\n",
            "  Training RandomForest...\n",
            "    RandomForest - F1: 0.7513, Precision: 1.0, Recall: 0.6016, AUC: 0.9959\n",
            "  Training XGBoost...\n",
            "    XGBoost - F1: 0.6409, Precision: 1.0, Recall: 0.4715, AUC: 0.8706\n",
            "\n",
            "Applying CTGAN augmentation...\n",
            "  Augmented training shape: (98558, 29)\n",
            "  Augmented class distribution: [80400 18158]\n",
            "  Training LogisticRegression...\n",
            "    LogisticRegression - F1: 0.1074, Precision: 0.3077, Recall: 0.065, AUC: 0.7605\n",
            "  Training RandomForest...\n",
            "    RandomForest - F1: 0.6102, Precision: 1.0, Recall: 0.439, AUC: 0.9593\n",
            "  Training XGBoost...\n",
            "    XGBoost - F1: 0.4625, Precision: 1.0, Recall: 0.3008, AUC: 0.8744\n",
            "\n",
            "--- Running V2_Tuned ---\n",
            "\n",
            "Applying SMOTE augmentation...\n",
            "  Augmented training shape: (73918, 29)\n",
            "  Augmented class distribution: [49279 24639]\n",
            "  Training LogisticRegression...\n",
            "    LogisticRegression - F1: 0.1249, Precision: 0.0691, Recall: 0.6504, AUC: 0.8602\n",
            "  Training RandomForest...\n",
            "    RandomForest - F1: 0.7513, Precision: 1.0, Recall: 0.6016, AUC: 0.9959\n",
            "  Training XGBoost...\n",
            "    XGBoost - F1: 0.6023, Precision: 1.0, Recall: 0.4309, AUC: 0.8761\n",
            "\n",
            "Applying ADASYN augmentation...\n",
            "  Augmented training shape: (73984, 29)\n",
            "  Augmented class distribution: [49279 24705]\n",
            "  Training LogisticRegression...\n",
            "    LogisticRegression - F1: 0.1183, Precision: 0.065, Recall: 0.6504, AUC: 0.8616\n",
            "  Training RandomForest...\n",
            "    RandomForest - F1: 0.7576, Precision: 1.0, Recall: 0.6098, AUC: 0.9959\n",
            "  Training XGBoost...\n",
            "    XGBoost - F1: 0.5862, Precision: 1.0, Recall: 0.4146, AUC: 0.8634\n",
            "\n",
            "Applying CTGAN augmentation...\n",
            "  Augmented training shape: (98558, 29)\n",
            "  Augmented class distribution: [80297 18261]\n",
            "  Training LogisticRegression...\n",
            "    LogisticRegression - F1: 0.1258, Precision: 0.0785, Recall: 0.3171, AUC: 0.7969\n",
            "  Training RandomForest...\n",
            "    RandomForest - F1: 0.6257, Precision: 1.0, Recall: 0.4553, AUC: 0.9675\n",
            "  Training XGBoost...\n",
            "    XGBoost - F1: 0.5091, Precision: 1.0, Recall: 0.3415, AUC: 0.8846\n",
            "\n",
            "\n",
            "================================================================================\n",
            "COMPARISON TABLE: Breast Cancer - Ratio 1:100\n",
            "================================================================================\n",
            "Variation                   V1_Default                           V2_Tuned                          \n",
            "Metric                              F1 Precision  Recall AUC-ROC       F1 Precision  Recall AUC-ROC\n",
            "Method   Classifier                                                                                \n",
            "Baseline LogisticRegression     0.2553       1.0  0.1463  0.8604   0.2553       1.0  0.1463  0.8604\n",
            "         RandomForest           0.6023       1.0  0.4309  0.9837   0.6023       1.0  0.4309  0.9837\n",
            "         XGBoost                0.6023       1.0  0.4309  0.8688   0.6023       1.0  0.4309  0.8688\n",
            "SMOTE    LogisticRegression     0.0729    0.0383  0.7398  0.8587   0.1249    0.0691  0.6504  0.8602\n",
            "         RandomForest           0.7513       1.0  0.6016     1.0   0.7513       1.0  0.6016  0.9959\n",
            "         XGBoost                0.6484       1.0  0.4797  0.8616   0.6023       1.0  0.4309  0.8761\n",
            "ADASYN   LogisticRegression     0.0685    0.0359   0.748  0.8588   0.1183     0.065  0.6504  0.8616\n",
            "         RandomForest           0.7513       1.0  0.6016  0.9959   0.7576       1.0  0.6098  0.9959\n",
            "         XGBoost                0.6409       1.0  0.4715  0.8706   0.5862       1.0  0.4146  0.8634\n",
            "CTGAN    LogisticRegression     0.1074    0.3077   0.065  0.7605   0.1258    0.0785  0.3171  0.7969\n",
            "         RandomForest           0.6102       1.0   0.439  0.9593   0.6257       1.0  0.4553  0.9675\n",
            "         XGBoost                0.4625       1.0  0.3008  0.8744   0.5091       1.0  0.3415  0.8846\n",
            "================================================================================\n",
            "\n",
            "============================================================\n",
            "RUNNING EXPERIMENT: Credit Card - Ratio 1:10\n",
            "============================================================\n",
            "Dataset class distribution: {0: np.int64(730), 1: np.int64(73)}\n",
            "Minority class: 1 (73 samples)\n",
            "Majority class: 0 (730 samples)\n",
            "Train shape: (642, 30), Test shape: (161, 30)\n",
            "Train class distribution: [584  58]\n",
            "Test class distribution: [146  15]\n",
            "\n",
            "--- Running Baseline (No Augmentation) ---\n",
            "Training LogisticRegression...\n",
            "  LogisticRegression - F1: 0.8889, Precision: 1.0, Recall: 0.8, AUC: 0.9753\n",
            "Training RandomForest...\n",
            "  RandomForest - F1: 0.9286, Precision: 1.0, Recall: 0.8667, AUC: 0.9858\n",
            "Training XGBoost...\n",
            "  XGBoost - F1: 0.9286, Precision: 1.0, Recall: 0.8667, AUC: 0.9959\n",
            "\n",
            "--- Running V1_Default ---\n",
            "\n",
            "Applying SMOTE augmentation...\n",
            "  Augmented training shape: (1168, 30)\n",
            "  Augmented class distribution: [584 584]\n",
            "  Training LogisticRegression...\n",
            "    LogisticRegression - F1: 0.8889, Precision: 1.0, Recall: 0.8, AUC: 0.9694\n",
            "  Training RandomForest...\n",
            "    RandomForest - F1: 0.9286, Precision: 1.0, Recall: 0.8667, AUC: 0.9929\n",
            "  Training XGBoost...\n",
            "    XGBoost - F1: 0.8966, Precision: 0.9286, Recall: 0.8667, AUC: 0.9959\n",
            "\n",
            "Applying ADASYN augmentation...\n",
            "  Augmented training shape: (1168, 30)\n",
            "  Augmented class distribution: [584 584]\n",
            "  Training LogisticRegression...\n",
            "    LogisticRegression - F1: 0.8889, Precision: 1.0, Recall: 0.8, AUC: 0.9493\n",
            "  Training RandomForest...\n",
            "    RandomForest - F1: 0.9286, Precision: 1.0, Recall: 0.8667, AUC: 0.9986\n",
            "  Training XGBoost...\n",
            "    XGBoost - F1: 0.9655, Precision: 1.0, Recall: 0.9333, AUC: 0.9991\n",
            "\n",
            "Applying CTGAN augmentation...\n",
            "  Augmented training shape: (1168, 30)\n",
            "  Augmented class distribution: [881 287]\n",
            "  Training LogisticRegression...\n",
            "    LogisticRegression - F1: 0.6957, Precision: 1.0, Recall: 0.5333, AUC: 0.9342\n",
            "  Training RandomForest...\n",
            "    RandomForest - F1: 0.8889, Precision: 1.0, Recall: 0.8, AUC: 0.9703\n",
            "  Training XGBoost...\n",
            "    XGBoost - F1: 0.8889, Precision: 1.0, Recall: 0.8, AUC: 0.9968\n",
            "\n",
            "--- Running V2_Tuned ---\n",
            "\n",
            "Applying SMOTE augmentation...\n",
            "  Augmented training shape: (876, 30)\n",
            "  Augmented class distribution: [584 292]\n",
            "  Training LogisticRegression...\n",
            "    LogisticRegression - F1: 0.8889, Precision: 1.0, Recall: 0.8, AUC: 0.9511\n",
            "  Training RandomForest...\n",
            "    RandomForest - F1: 0.9286, Precision: 1.0, Recall: 0.8667, AUC: 0.9973\n",
            "  Training XGBoost...\n",
            "    XGBoost - F1: 0.9286, Precision: 1.0, Recall: 0.8667, AUC: 0.9954\n",
            "\n",
            "Applying ADASYN augmentation...\n",
            "  Augmented training shape: (876, 30)\n",
            "  Augmented class distribution: [584 292]\n",
            "  Training LogisticRegression...\n",
            "    LogisticRegression - F1: 0.8889, Precision: 1.0, Recall: 0.8, AUC: 0.9447\n",
            "  Training RandomForest...\n",
            "    RandomForest - F1: 0.9286, Precision: 1.0, Recall: 0.8667, AUC: 0.997\n",
            "  Training XGBoost...\n",
            "    XGBoost - F1: 0.9286, Precision: 1.0, Recall: 0.8667, AUC: 0.9991\n",
            "\n",
            "Applying CTGAN augmentation...\n",
            "  Augmented training shape: (1168, 30)\n",
            "  Augmented class distribution: [890 278]\n",
            "  Training LogisticRegression...\n",
            "    LogisticRegression - F1: 0.8, Precision: 1.0, Recall: 0.6667, AUC: 0.8886\n",
            "  Training RandomForest...\n",
            "    RandomForest - F1: 0.9286, Precision: 1.0, Recall: 0.8667, AUC: 0.971\n",
            "  Training XGBoost...\n",
            "    XGBoost - F1: 0.9286, Precision: 1.0, Recall: 0.8667, AUC: 0.9922\n",
            "\n",
            "\n",
            "================================================================================\n",
            "COMPARISON TABLE: Credit Card - Ratio 1:10\n",
            "================================================================================\n",
            "Variation                   V1_Default                           V2_Tuned                          \n",
            "Metric                              F1 Precision  Recall AUC-ROC       F1 Precision  Recall AUC-ROC\n",
            "Method   Classifier                                                                                \n",
            "Baseline LogisticRegression     0.8889       1.0     0.8  0.9753   0.8889       1.0     0.8  0.9753\n",
            "         RandomForest           0.9286       1.0  0.8667  0.9858   0.9286       1.0  0.8667  0.9858\n",
            "         XGBoost                0.9286       1.0  0.8667  0.9959   0.9286       1.0  0.8667  0.9959\n",
            "SMOTE    LogisticRegression     0.8889       1.0     0.8  0.9694   0.8889       1.0     0.8  0.9511\n",
            "         RandomForest           0.9286       1.0  0.8667  0.9929   0.9286       1.0  0.8667  0.9973\n",
            "         XGBoost                0.8966    0.9286  0.8667  0.9959   0.9286       1.0  0.8667  0.9954\n",
            "ADASYN   LogisticRegression     0.8889       1.0     0.8  0.9493   0.8889       1.0     0.8  0.9447\n",
            "         RandomForest           0.9286       1.0  0.8667  0.9986   0.9286       1.0  0.8667   0.997\n",
            "         XGBoost                0.9655       1.0  0.9333  0.9991   0.9286       1.0  0.8667  0.9991\n",
            "CTGAN    LogisticRegression     0.6957       1.0  0.5333  0.9342      0.8       1.0  0.6667  0.8886\n",
            "         RandomForest           0.8889       1.0     0.8  0.9703   0.9286       1.0  0.8667   0.971\n",
            "         XGBoost                0.8889       1.0     0.8  0.9968   0.9286       1.0  0.8667  0.9922\n",
            "================================================================================\n",
            "\n",
            "============================================================\n",
            "RUNNING EXPERIMENT: Credit Card - Ratio 1:100\n",
            "============================================================\n",
            "Dataset class distribution: {0: np.int64(7300), 1: np.int64(73)}\n",
            "Minority class: 1 (73 samples)\n",
            "Majority class: 0 (7300 samples)\n",
            "Train shape: (5898, 30), Test shape: (1475, 30)\n",
            "Train class distribution: [5840   58]\n",
            "Test class distribution: [1460   15]\n",
            "\n",
            "--- Running Baseline (No Augmentation) ---\n",
            "Training LogisticRegression...\n",
            "  LogisticRegression - F1: 0.8667, Precision: 0.8667, Recall: 0.8667, AUC: 0.9986\n",
            "Training RandomForest...\n",
            "  RandomForest - F1: 0.9286, Precision: 1.0, Recall: 0.8667, AUC: 0.9998\n",
            "Training XGBoost...\n",
            "  XGBoost - F1: 0.8889, Precision: 1.0, Recall: 0.8, AUC: 1.0\n",
            "\n",
            "--- Running V1_Default ---\n",
            "\n",
            "Applying SMOTE augmentation...\n",
            "  Augmented training shape: (11680, 30)\n",
            "  Augmented class distribution: [5840 5840]\n",
            "  Training LogisticRegression...\n",
            "    LogisticRegression - F1: 0.625, Precision: 0.4545, Recall: 1.0, AUC: 0.998\n",
            "  Training RandomForest...\n",
            "    RandomForest - F1: 0.9286, Precision: 1.0, Recall: 0.8667, AUC: 1.0\n",
            "  Training XGBoost...\n",
            "    XGBoost - F1: 1.0, Precision: 1.0, Recall: 1.0, AUC: 1.0\n",
            "\n",
            "Applying ADASYN augmentation...\n",
            "  Augmented training shape: (11680, 30)\n",
            "  Augmented class distribution: [5840 5840]\n",
            "  Training LogisticRegression...\n",
            "    LogisticRegression - F1: 0.4407, Precision: 0.2955, Recall: 0.8667, AUC: 0.9944\n",
            "  Training RandomForest...\n",
            "    RandomForest - F1: 1.0, Precision: 1.0, Recall: 1.0, AUC: 1.0\n",
            "  Training XGBoost...\n",
            "    XGBoost - F1: 0.9333, Precision: 0.9333, Recall: 0.9333, AUC: 1.0\n",
            "\n",
            "Applying CTGAN augmentation...\n",
            "  Augmented training shape: (11680, 30)\n",
            "  Augmented class distribution: [9758 1922]\n",
            "  Training LogisticRegression...\n",
            "    LogisticRegression - F1: 0.7692, Precision: 0.9091, Recall: 0.6667, AUC: 0.9987\n",
            "  Training RandomForest...\n",
            "    RandomForest - F1: 0.8966, Precision: 0.9286, Recall: 0.8667, AUC: 0.9999\n",
            "  Training XGBoost...\n",
            "    XGBoost - F1: 0.9286, Precision: 1.0, Recall: 0.8667, AUC: 1.0\n",
            "\n",
            "--- Running V2_Tuned ---\n",
            "\n",
            "Applying SMOTE augmentation...\n",
            "  Augmented training shape: (8760, 30)\n",
            "  Augmented class distribution: [5840 2920]\n",
            "  Training LogisticRegression...\n",
            "    LogisticRegression - F1: 0.6818, Precision: 0.5172, Recall: 1.0, AUC: 0.9988\n",
            "  Training RandomForest...\n",
            "    RandomForest - F1: 0.9286, Precision: 1.0, Recall: 0.8667, AUC: 1.0\n",
            "  Training XGBoost...\n",
            "    XGBoost - F1: 0.9655, Precision: 1.0, Recall: 0.9333, AUC: 1.0\n",
            "\n",
            "Applying ADASYN augmentation...\n",
            "  Augmented training shape: (8758, 30)\n",
            "  Augmented class distribution: [5840 2918]\n",
            "  Training LogisticRegression...\n",
            "    LogisticRegression - F1: 0.625, Precision: 0.4545, Recall: 1.0, AUC: 0.9977\n",
            "  Training RandomForest...\n",
            "    RandomForest - F1: 0.9677, Precision: 0.9375, Recall: 1.0, AUC: 1.0\n",
            "  Training XGBoost...\n",
            "    XGBoost - F1: 1.0, Precision: 1.0, Recall: 1.0, AUC: 1.0\n",
            "\n",
            "Applying CTGAN augmentation...\n",
            "  Augmented training shape: (11680, 30)\n",
            "  Augmented class distribution: [9776 1904]\n",
            "  Training LogisticRegression...\n",
            "    LogisticRegression - F1: 0.7692, Precision: 0.9091, Recall: 0.6667, AUC: 0.9862\n",
            "  Training RandomForest...\n",
            "    RandomForest - F1: 0.9286, Precision: 1.0, Recall: 0.8667, AUC: 1.0\n",
            "  Training XGBoost...\n",
            "    XGBoost - F1: 0.9286, Precision: 1.0, Recall: 0.8667, AUC: 0.9997\n",
            "\n",
            "\n",
            "================================================================================\n",
            "COMPARISON TABLE: Credit Card - Ratio 1:100\n",
            "================================================================================\n",
            "Variation                   V1_Default                           V2_Tuned                          \n",
            "Metric                              F1 Precision  Recall AUC-ROC       F1 Precision  Recall AUC-ROC\n",
            "Method   Classifier                                                                                \n",
            "Baseline LogisticRegression     0.8667    0.8667  0.8667  0.9986   0.8667    0.8667  0.8667  0.9986\n",
            "         RandomForest           0.9286       1.0  0.8667  0.9998   0.9286       1.0  0.8667  0.9998\n",
            "         XGBoost                0.8889       1.0     0.8     1.0   0.8889       1.0     0.8     1.0\n",
            "SMOTE    LogisticRegression      0.625    0.4545     1.0   0.998   0.6818    0.5172     1.0  0.9988\n",
            "         RandomForest           0.9286       1.0  0.8667     1.0   0.9286       1.0  0.8667     1.0\n",
            "         XGBoost                   1.0       1.0     1.0     1.0   0.9655       1.0  0.9333     1.0\n",
            "ADASYN   LogisticRegression     0.4407    0.2955  0.8667  0.9944    0.625    0.4545     1.0  0.9977\n",
            "         RandomForest              1.0       1.0     1.0     1.0   0.9677    0.9375     1.0     1.0\n",
            "         XGBoost                0.9333    0.9333  0.9333     1.0      1.0       1.0     1.0     1.0\n",
            "CTGAN    LogisticRegression     0.7692    0.9091  0.6667  0.9987   0.7692    0.9091  0.6667  0.9862\n",
            "         RandomForest           0.8966    0.9286  0.8667  0.9999   0.9286       1.0  0.8667     1.0\n",
            "         XGBoost                0.9286       1.0  0.8667     1.0   0.9286       1.0  0.8667  0.9997\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "def create_comparison_table(results, dataset_name, ratio):\n",
        "    \"\"\"\n",
        "    Create comparison table combining V1 and V2 results for one ratio\n",
        "    \"\"\"\n",
        "    methods = ['Baseline', 'SMOTE', 'ADASYN', 'CTGAN']\n",
        "    classifiers = ['LogisticRegression', 'RandomForest', 'XGBoost']\n",
        "    metrics = ['F1', 'Precision', 'Recall', 'AUC-ROC']\n",
        "\n",
        "    # Create multi-index columns\n",
        "    columns = []\n",
        "    for variation in ['V1_Default', 'V2_Tuned']:\n",
        "        for metric in metrics:\n",
        "            columns.append((variation, metric))\n",
        "\n",
        "    multi_index = pd.MultiIndex.from_tuples(columns, names=['Variation', 'Metric'])\n",
        "\n",
        "    # Create row index\n",
        "    row_index = []\n",
        "    for method in methods:\n",
        "        for clf in classifiers:\n",
        "            row_index.append((method, clf))\n",
        "\n",
        "    multi_row_index = pd.MultiIndex.from_tuples(row_index, names=['Method', 'Classifier'])\n",
        "\n",
        "    # Create DataFrame\n",
        "    comparison_df = pd.DataFrame(index=multi_row_index, columns=multi_index)\n",
        "\n",
        "    # Fill data\n",
        "    for variation in ['V1_Default', 'V2_Tuned']:\n",
        "        for method in methods:\n",
        "            for clf in classifiers:\n",
        "                for metric in metrics:\n",
        "                    try:\n",
        "                        value = results[variation][method][clf][metric]\n",
        "                        comparison_df.loc[(method, clf), (variation, metric)] = value\n",
        "                    except KeyError:\n",
        "                        comparison_df.loc[(method, clf), (variation, metric)] = np.nan\n",
        "\n",
        "    return comparison_df\n",
        "\n",
        "# Run all experiments\n",
        "all_results = {}\n",
        "datasets_info = [\n",
        "    ('breast_cancer', 'Status', 'Breast Cancer'),\n",
        "    ('creditcard', 'Class', 'Credit Card')\n",
        "]\n",
        "\n",
        "for dataset_prefix, target_col, dataset_display_name in datasets_info:\n",
        "    all_results[dataset_prefix] = {}\n",
        "\n",
        "    for ratio in [10, 100]:\n",
        "        # Load imbalanced dataset\n",
        "        filename = f\"imbalanced_datasets/{dataset_prefix}_1_{ratio}.csv\"\n",
        "        df = pd.read_csv(filename)\n",
        "\n",
        "        # Ensure target column is numerical (0 and 1) and encode categorical features\n",
        "        if dataset_prefix == 'breast_cancer':\n",
        "            df[target_col] = df[target_col].map({'Alive': 0, 'Dead': 1}).astype(int)\n",
        "            # Dynamically identify categorical features (object dtype)\n",
        "            categorical_cols_to_encode = df.select_dtypes(include='object').columns.tolist()\n",
        "            # Ensure target_col is not in the list to be encoded if it somehow got there\n",
        "            if target_col in categorical_cols_to_encode:\n",
        "                categorical_cols_to_encode.remove(target_col)\n",
        "            df = pd.get_dummies(df, columns=categorical_cols_to_encode, drop_first=True)\n",
        "        elif dataset_prefix == 'creditcard':\n",
        "            df[target_col] = df[target_col].astype(int) # Ensure it's int just in case\n",
        "            # Credit card data is already numerical, no additional feature encoding needed.\n",
        "\n",
        "        # Run experiment\n",
        "        results = run_experiment(df, target_col, dataset_display_name, ratio)\n",
        "        all_results[dataset_prefix][f\"1_{ratio}\"] = results\n",
        "\n",
        "        # Create and display comparison table\n",
        "        comparison_table = create_comparison_table(results, dataset_display_name, ratio)\n",
        "\n",
        "        print(f\"\\n\\n{'='*80}\")\n",
        "        print(f\"COMPARISON TABLE: {dataset_display_name} - Ratio 1:{ratio}\")\n",
        "        print(f\"{'='*80}\")\n",
        "        print(comparison_table.to_string())\n",
        "        print(f\"{'='*80}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18",
      "metadata": {
        "id": "18"
      },
      "source": [
        "## Step 6: Save Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "19",
      "metadata": {
        "id": "19",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0cf7265-54e7-4216-9d0f-873580aecc55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "SAVING RESULTS\n",
            "============================================================\n",
            "Saved: results/breast_cancer_results.xlsx\n",
            "Saved: results/creditcard_results.xlsx\n",
            "Saved: results/all_results_summary.xlsx\n",
            "Saved: experiment_config.json\n",
            "\n",
            "All results saved successfully!\n"
          ]
        }
      ],
      "source": [
        "def save_results_to_excel(all_results):\n",
        "    \"\"\"\n",
        "    Save results to Excel files with multiple sheets\n",
        "    \"\"\"\n",
        "    # Create the 'results' directory if it doesn't exist\n",
        "    os.makedirs('results', exist_ok=True)\n",
        "\n",
        "    # Save individual dataset results\n",
        "    for dataset_name, dataset_results in all_results.items():\n",
        "        filename = f\"results/{dataset_name}_results.xlsx\"\n",
        "\n",
        "        with pd.ExcelWriter(filename, engine='openpyxl') as writer:\n",
        "            for ratio_name, results in dataset_results.items():\n",
        "                ratio_num = ratio_name.split('_')[1]\n",
        "                dataset_display = 'Breast Cancer' if dataset_name == 'breast_cancer' else 'Credit Card'\n",
        "\n",
        "                comparison_table = create_comparison_table(results, dataset_display, int(ratio_num))\n",
        "                comparison_table.to_excel(writer, sheet_name=f\"Ratio_1_{ratio_num}\")\n",
        "\n",
        "        print(f\"Saved: {filename}\")\n",
        "\n",
        "    # Save combined results\n",
        "    with pd.ExcelWriter('results/all_results_summary.xlsx', engine='openpyxl') as writer:\n",
        "        for dataset_name, dataset_results in all_results.items():\n",
        "            for ratio_name, results in dataset_results.items():\n",
        "                ratio_num = ratio_name.split('_')[1]\n",
        "                dataset_display = 'Breast Cancer' if dataset_name == 'breast_cancer' else 'Credit Card'\n",
        "\n",
        "                comparison_table = create_comparison_table(results, dataset_display, int(ratio_num))\n",
        "                sheet_name = f\"{dataset_name}_1_{ratio_num}\"\n",
        "                comparison_table.to_excel(writer, sheet_name=sheet_name)\n",
        "\n",
        "    print(\"Saved: results/all_results_summary.xlsx\")\n",
        "\n",
        "def save_experiment_config():\n",
        "    \"\"\"\n",
        "    Save experiment configuration to JSON\n",
        "    \"\"\"\n",
        "    config = {\n",
        "        'datasets': ['breast_cancer', 'creditcard'],\n",
        "        'imbalance_ratios': [10, 100],\n",
        "        'classifiers': {\n",
        "            'LogisticRegression': {'max_iter': 1000, 'random_state': 42},\n",
        "            'RandomForest': {'n_estimators': 100, 'random_state': 42},\n",
        "            'XGBoost': {'n_estimators': 100, 'eval_metric': 'logloss', 'random_state': 42, 'use_label_encoder': False}\n",
        "        },\n",
        "        'augmentation_configs': get_augmentation_configs(),\n",
        "        'train_test_split': {'test_size': 0.2, 'stratify': True, 'random_state': 42},\n",
        "        'metrics': ['F1', 'Precision', 'Recall', 'AUC-ROC'],\n",
        "        'pos_label': 1\n",
        "    }\n",
        "\n",
        "    with open('experiment_config.json', 'w') as f:\n",
        "        json.dump(config, f, indent=2)\n",
        "\n",
        "    print(\"Saved: experiment_config.json\")\n",
        "\n",
        "# Save all results\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"SAVING RESULTS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "save_results_to_excel(all_results)\n",
        "save_experiment_config()\n",
        "\n",
        "print(\"\\nAll results saved successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20",
      "metadata": {
        "id": "20"
      },
      "source": [
        "## Download Results"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display\n",
        "\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "display(results_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "KY5Py9oAntiE",
        "outputId": "96be342f-705e-4af8-9665-a1d01619a91c"
      },
      "id": "KY5Py9oAntiE",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                                 V1_Default  \\\n",
              "Baseline  {'LogisticRegression': {'F1': 0.8667, 'Precisi...   \n",
              "SMOTE     {'LogisticRegression': {'F1': 0.625, 'Precisio...   \n",
              "ADASYN    {'LogisticRegression': {'F1': 0.4407, 'Precisi...   \n",
              "CTGAN     {'LogisticRegression': {'F1': 0.7692, 'Precisi...   \n",
              "\n",
              "                                                   V2_Tuned  \n",
              "Baseline  {'LogisticRegression': {'F1': 0.8667, 'Precisi...  \n",
              "SMOTE     {'LogisticRegression': {'F1': 0.6818, 'Precisi...  \n",
              "ADASYN    {'LogisticRegression': {'F1': 0.625, 'Precisio...  \n",
              "CTGAN     {'LogisticRegression': {'F1': 0.7692, 'Precisi...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e475298b-6b6c-4ca9-9abe-b50b4e1d32e2\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>V1_Default</th>\n",
              "      <th>V2_Tuned</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Baseline</th>\n",
              "      <td>{'LogisticRegression': {'F1': 0.8667, 'Precisi...</td>\n",
              "      <td>{'LogisticRegression': {'F1': 0.8667, 'Precisi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SMOTE</th>\n",
              "      <td>{'LogisticRegression': {'F1': 0.625, 'Precisio...</td>\n",
              "      <td>{'LogisticRegression': {'F1': 0.6818, 'Precisi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ADASYN</th>\n",
              "      <td>{'LogisticRegression': {'F1': 0.4407, 'Precisi...</td>\n",
              "      <td>{'LogisticRegression': {'F1': 0.625, 'Precisio...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>CTGAN</th>\n",
              "      <td>{'LogisticRegression': {'F1': 0.7692, 'Precisi...</td>\n",
              "      <td>{'LogisticRegression': {'F1': 0.7692, 'Precisi...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e475298b-6b6c-4ca9-9abe-b50b4e1d32e2')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e475298b-6b6c-4ca9-9abe-b50b4e1d32e2 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e475298b-6b6c-4ca9-9abe-b50b4e1d32e2');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "  <div id=\"id_048ec762-e633-4428-9aa0-da71a37f8b7b\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('results_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_048ec762-e633-4428-9aa0-da71a37f8b7b button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('results_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "results_df",
              "summary": "{\n  \"name\": \"results_df\",\n  \"rows\": 4,\n  \"fields\": [\n    {\n      \"column\": \"V1_Default\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"V2_Tuned\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22",
      "metadata": {
        "id": "22"
      },
      "source": [
        "**Dataset Summary:**\n",
        "- **Breast Cancer**: 0=alive (majority), 1=dead (minority) - using `status` column\n",
        "- **Credit Card**: 0=normal (majority), 1=fraud (minority) - using `Class` column"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}