{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abinayanand7896-cloud/Abinaya_Anand/blob/main/data_augmentation_comparison.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2",
        "outputId": "06565713-ea84-44b0-d25e-8b82c99ce211"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.12/dist-packages (0.14.1)\n",
            "Collecting ctgan\n",
            "  Using cached ctgan-0.12.1-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting sdv\n",
            "  Using cached sdv-1.34.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.12/dist-packages (3.2.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.12/dist-packages (3.1.5)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: sklearn-compat<0.2,>=0.1.5 in /usr/local/lib/python3.12/dist-packages (from imbalanced-learn) (0.1.5)\n",
            "Requirement already satisfied: torch>=2.3.0 in /usr/local/lib/python3.12/dist-packages (from ctgan) (2.10.0+cu128)\n",
            "Requirement already satisfied: tqdm>=4.29 in /usr/local/lib/python3.12/dist-packages (from ctgan) (4.67.3)\n",
            "Requirement already satisfied: rdt>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from ctgan) (1.20.0)\n",
            "Collecting boto3<2.0.0,>=1.28 (from sdv)\n",
            "  Using cached boto3-1.42.56-py3-none-any.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: botocore<2.0.0,>=1.31 in /usr/local/lib/python3.12/dist-packages (from sdv) (1.42.56)\n",
            "Requirement already satisfied: cloudpickle>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from sdv) (3.1.2)\n",
            "Requirement already satisfied: graphviz>=0.13.2 in /usr/local/lib/python3.12/dist-packages (from sdv) (0.21)\n",
            "Collecting copulas>=0.12.1 (from sdv)\n",
            "  Using cached copulas-0.14.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting deepecho>=0.7.0 (from sdv)\n",
            "  Using cached deepecho-0.8.1-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting sdmetrics>=0.21.0 (from sdv)\n",
            "  Using cached sdmetrics-0.27.1-py3-none-any.whl.metadata (10.0 kB)\n",
            "Requirement already satisfied: platformdirs>=4.0 in /usr/local/lib/python3.12/dist-packages (from sdv) (4.9.2)\n",
            "Requirement already satisfied: pyyaml>=6.0.1 in /usr/local/lib/python3.12/dist-packages (from sdv) (6.0.3)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.12/dist-packages (from xgboost) (2.27.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.3)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.12/dist-packages (from openpyxl) (2.0.0)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from boto3<2.0.0,>=1.28->sdv) (1.1.0)\n",
            "Requirement already satisfied: s3transfer<0.17.0,>=0.16.0 in /usr/local/lib/python3.12/dist-packages (from boto3<2.0.0,>=1.28->sdv) (0.16.0)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.12/dist-packages (from botocore<2.0.0,>=1.31->sdv) (2.5.0)\n",
            "Requirement already satisfied: plotly>=5.10.0 in /usr/local/lib/python3.12/dist-packages (from copulas>=0.12.1->sdv) (5.24.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: Faker!=37.11.0,>=17 in /usr/local/lib/python3.12/dist-packages (from rdt>=1.14.0->ctgan) (40.5.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->ctgan) (3.24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->ctgan) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->ctgan) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->ctgan) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->ctgan) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->ctgan) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->ctgan) (2025.3.0)\n",
            "Requirement already satisfied: cuda-bindings==12.9.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->ctgan) (12.9.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->ctgan) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->ctgan) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->ctgan) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->ctgan) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->ctgan) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->ctgan) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->ctgan) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->ctgan) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->ctgan) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->ctgan) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.4.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->ctgan) (3.4.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->ctgan) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->ctgan) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->ctgan) (1.13.1.3)\n",
            "Requirement already satisfied: triton==3.6.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->ctgan) (3.6.0)\n",
            "Requirement already satisfied: cuda-pathfinder~=1.1 in /usr/local/lib/python3.12/dist-packages (from cuda-bindings==12.9.4->torch>=2.3.0->ctgan) (1.3.4)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly>=5.10.0->copulas>=0.12.1->sdv) (9.1.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from plotly>=5.10.0->copulas>=0.12.1->sdv) (26.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.3.0->ctgan) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.3.0->ctgan) (3.0.3)\n",
            "Using cached ctgan-0.12.1-py3-none-any.whl (25 kB)\n",
            "Using cached sdv-1.34.1-py3-none-any.whl (200 kB)\n",
            "Using cached boto3-1.42.56-py3-none-any.whl (140 kB)\n",
            "Using cached copulas-0.14.1-py3-none-any.whl (52 kB)\n",
            "Using cached deepecho-0.8.1-py3-none-any.whl (28 kB)\n",
            "Using cached sdmetrics-0.27.1-py3-none-any.whl (201 kB)\n",
            "Installing collected packages: copulas, sdmetrics, deepecho, ctgan, boto3, sdv\n",
            "Successfully installed boto3-1.42.56 copulas-0.14.1 ctgan-0.12.1 deepecho-0.8.1 sdmetrics-0.27.1 sdv-1.34.1\n"
          ]
        }
      ],
      "source": [
        "!pip install scikit-learn imbalanced-learn ctgan sdv xgboost pandas numpy openpyxl"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3",
      "metadata": {
        "id": "3"
      },
      "source": [
        "## Import Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "4",
      "metadata": {
        "id": "4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import json\n",
        "import warnings\n",
        "from collections import defaultdict\n",
        "import zipfile\n",
        "\n",
        "# ML Libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Augmentation techniques\n",
        "from imblearn.over_sampling import SMOTE, ADASYN\n",
        "from ctgan import CTGAN\n",
        "\n",
        "# Metrics\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score\n",
        "\n",
        "# Google Colab utilities\n",
        "from google.colab import files\n",
        "from google.colab import drive\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5",
      "metadata": {
        "id": "5"
      },
      "source": [
        "## Data Loading and Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6",
        "outputId": "bd1dc822-1970-4cfe-85c4-5c5f79582ffd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cancer Diagnosis shape: (4024, 16)\n",
            "Cancer target: 'Status'\n",
            "Class distribution:\n",
            "Status\n",
            "Alive    3408\n",
            "Dead      616\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "cancer_df = pd.read_csv('/content/Breast_Cancer.csv')\n",
        "print(f\"Cancer Diagnosis shape: {cancer_df.shape}\")\n",
        "\n",
        "cancer_target = 'Status' if 'Status' in cancer_df.columns else cancer_df.columns[-1]\n",
        "print(f\"Cancer target: '{cancer_target}'\")\n",
        "print(f\"Class distribution:\\n{cancer_df[cancer_target].value_counts()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "3YXfQrhh2_tq",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3YXfQrhh2_tq",
        "outputId": "92faa57c-e8ff-4bdc-dd01-c1ca7982d3cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fraud Detection shape: (284807, 31)\n",
            "Fraud target: 'Class'\n",
            "Class distribution:\n",
            "Class\n",
            "0    284315\n",
            "1       492\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "fraud_df = pd.read_csv('/content/creditcard.csv')\n",
        "print(f\"Fraud Detection shape: {fraud_df.shape}\")\n",
        "\n",
        "# Identify target column (last column or 'Class')\n",
        "fraud_target = 'Class' if 'Class' in fraud_df.columns else fraud_df.columns[-1]\n",
        "print(f\"Fraud target: '{fraud_target}'\")\n",
        "print(f\"Class distribution:\\n{fraud_df[fraud_target].value_counts()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8",
      "metadata": {
        "id": "8"
      },
      "source": [
        "## Step 1: Create Imbalanced Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9",
        "outputId": "3f940f70-e1b6-4f56-9f5a-13593e4e5a0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "CREATING BREAST CANCER IMBALANCED DATASETS\n",
            "==================================================\n",
            "\n",
            "Creating Breast Cancer 1:10 with 1:10 ratio\n",
            "Minority class (Dead): 616 samples\n",
            "Target majority samples: 6160\n",
            "Bootstrapped majority class to 6160 samples (repeated 2 times)\n",
            "Final dataset shape: (6776, 16)\n",
            "Final class distribution: {'Alive': 6160, 'Dead': 616}\n",
            "Saved: imbalanced_datasets/breast_cancer_1_10.csv\n",
            "\n",
            "Creating Breast Cancer 1:100 with 1:100 ratio\n",
            "Minority class (Dead): 616 samples\n",
            "Target majority samples: 61600\n",
            "Bootstrapped majority class to 61600 samples (repeated 19 times)\n",
            "Final dataset shape: (62216, 16)\n",
            "Final class distribution: {'Alive': 61600, 'Dead': 616}\n",
            "Saved: imbalanced_datasets/breast_cancer_1_100.csv\n",
            "\n",
            "==================================================\n",
            "CREATING CREDIT CARD IMBALANCED DATASETS\n",
            "==================================================\n",
            "\n",
            "Creating Credit Card 1:10 with 1:10 ratio\n",
            "Minority class (1): 492 samples\n",
            "Target majority samples: 4920\n",
            "Subsampled majority class to 4920 samples\n",
            "Final dataset shape: (5412, 31)\n",
            "Final class distribution: {0: 4920, 1: 492}\n",
            "Saved: imbalanced_datasets/creditcard_1_10.csv\n",
            "\n",
            "Creating Credit Card 1:100 with 1:100 ratio\n",
            "Minority class (1): 492 samples\n",
            "Target majority samples: 49200\n",
            "Subsampled majority class to 49200 samples\n",
            "Final dataset shape: (49692, 31)\n",
            "Final class distribution: {0: 49200, 1: 492}\n",
            "Saved: imbalanced_datasets/creditcard_1_100.csv\n"
          ]
        }
      ],
      "source": [
        "def create_imbalanced_dataset(df, target_col, ratio, dataset_name):\n",
        "    \"\"\"\n",
        "    Create imbalanced dataset with specified minority:majority ratio\n",
        "    \"\"\"\n",
        "    minority_class = df[target_col].value_counts().idxmin()\n",
        "    majority_class = df[target_col].value_counts().idxmax()\n",
        "\n",
        "    minority_samples = df[df[target_col] == minority_class]\n",
        "    majority_samples = df[df[target_col] == majority_class]\n",
        "\n",
        "    n_minority = len(minority_samples)\n",
        "    n_majority_target = n_minority * ratio\n",
        "\n",
        "    print(f\"\\nCreating {dataset_name} with 1:{ratio} ratio\")\n",
        "    print(f\"Minority class ({minority_class}): {n_minority} samples\")\n",
        "    print(f\"Target majority samples: {n_majority_target}\")\n",
        "\n",
        "    # Handle different scenarios based on available majority samples\n",
        "    if len(majority_samples) >= n_majority_target:\n",
        "        # Subsample majority class\n",
        "        majority_selected = majority_samples.sample(n=n_majority_target, random_state=42)\n",
        "        print(f\"Subsampled majority class to {len(majority_selected)} samples\")\n",
        "    else:\n",
        "        # Bootstrap/duplicate majority class (for breast cancer with high ratios)\n",
        "        n_repeats = int(np.ceil(n_majority_target / len(majority_samples)))\n",
        "        majority_repeated = pd.concat([majority_samples] * n_repeats, ignore_index=True)\n",
        "        majority_selected = majority_repeated.sample(n=n_majority_target, random_state=42)\n",
        "        print(f\"Bootstrapped majority class to {len(majority_selected)} samples (repeated {n_repeats} times)\")\n",
        "\n",
        "    # Combine minority and majority samples\n",
        "    imbalanced_df = pd.concat([minority_samples, majority_selected], ignore_index=True)\n",
        "    imbalanced_df = imbalanced_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "    print(f\"Final dataset shape: {imbalanced_df.shape}\")\n",
        "    print(f\"Final class distribution: {imbalanced_df[target_col].value_counts().to_dict()}\")\n",
        "\n",
        "    return imbalanced_df\n",
        "\n",
        "# Create imbalanced datasets\n",
        "ratios = [10, 100]\n",
        "\n",
        "# Create directory if it doesn't exist\n",
        "os.makedirs('imbalanced_datasets', exist_ok=True)\n",
        "\n",
        "print(\"=\" * 50)\n",
        "print(\"CREATING BREAST CANCER IMBALANCED DATASETS\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "for ratio in ratios:\n",
        "    imbalanced_bc = create_imbalanced_dataset(cancer_df, 'Status', ratio, f\"Breast Cancer 1:{ratio}\")\n",
        "    filename = f\"imbalanced_datasets/breast_cancer_1_{ratio}.csv\"\n",
        "    imbalanced_bc.to_csv(filename, index=False)\n",
        "    print(f\"Saved: {filename}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"CREATING CREDIT CARD IMBALANCED DATASETS\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "for ratio in ratios:\n",
        "    imbalanced_cc = create_imbalanced_dataset(fraud_df, 'Class', ratio, f\"Credit Card 1:{ratio}\")\n",
        "    filename = f\"imbalanced_datasets/creditcard_1_{ratio}.csv\"\n",
        "    imbalanced_cc.to_csv(filename, index=False)\n",
        "    print(f\"Saved: {filename}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10",
      "metadata": {
        "id": "10"
      },
      "source": [
        "## Step 2: Define Fixed Classifiers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "11",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11",
        "outputId": "3e743ab7-f99b-4ffb-c038-4879fb188693"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fixed classifiers defined:\n",
            "- LogisticRegression: LogisticRegression(max_iter=1000, random_state=42)\n",
            "- RandomForest: RandomForestClassifier(random_state=42)\n",
            "- XGBoost: XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
            "              colsample_bylevel=None, colsample_bynode=None,\n",
            "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
            "              enable_categorical=False, eval_metric='logloss',\n",
            "              feature_types=None, feature_weights=None, gamma=None,\n",
            "              grow_policy=None, importance_type=None,\n",
            "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
            "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
            "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
            "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
            "              multi_strategy=None, n_estimators=100, n_jobs=None,\n",
            "              num_parallel_tree=None, ...)\n"
          ]
        }
      ],
      "source": [
        "def get_classifiers():\n",
        "    \"\"\"\n",
        "    Return fixed classifiers with specified parameters\n",
        "    \"\"\"\n",
        "    return {\n",
        "        'LogisticRegression': LogisticRegression(max_iter=1000, random_state=42),\n",
        "        'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "        'XGBoost': XGBClassifier(n_estimators=100, eval_metric='logloss',\n",
        "                                random_state=42, use_label_encoder=False)\n",
        "    }\n",
        "\n",
        "print(\"Fixed classifiers defined:\")\n",
        "for name, clf in get_classifiers().items():\n",
        "    print(f\"- {name}: {clf}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12",
      "metadata": {
        "id": "12"
      },
      "source": [
        "## Step 3: Define Augmentation Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "13",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13",
        "outputId": "11ee3625-b92d-4234-cad5-daa7d89f7529"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Augmentation configurations:\n",
            "\n",
            "V1_Default:\n",
            "  SMOTE: {'k_neighbors': 5, 'sampling_strategy': 'auto', 'random_state': 42}\n",
            "  ADASYN: {'n_neighbors': 5, 'sampling_strategy': 'auto', 'random_state': 42}\n",
            "  CTGAN: {'epochs': 100, 'batch_size': 500}\n",
            "\n",
            "V2_Tuned:\n",
            "  SMOTE: {'k_neighbors': 3, 'sampling_strategy': 0.5, 'random_state': 42}\n",
            "  ADASYN: {'n_neighbors': 10, 'sampling_strategy': 0.5, 'random_state': 42}\n",
            "  CTGAN: {'epochs': 150, 'batch_size': 100}\n"
          ]
        }
      ],
      "source": [
        "def get_augmentation_configs():\n",
        "    \"\"\"\n",
        "    Return augmentation configurations for V1 (Default) and V2 (Tuned)\n",
        "    \"\"\"\n",
        "    return {\n",
        "        'V1_Default': {\n",
        "            'SMOTE': {'k_neighbors': 5, 'sampling_strategy': 'auto', 'random_state': 42},\n",
        "            'ADASYN': {'n_neighbors': 5, 'sampling_strategy': 'auto', 'random_state': 42},\n",
        "            'CTGAN': {'epochs': 100, 'batch_size': 500}\n",
        "        },\n",
        "        'V2_Tuned': {\n",
        "            'SMOTE': {'k_neighbors': 3, 'sampling_strategy': 0.5, 'random_state': 42},\n",
        "            'ADASYN': {'n_neighbors': 10, 'sampling_strategy': 0.5, 'random_state': 42},\n",
        "            'CTGAN': {'epochs': 150, 'batch_size': 100}\n",
        "        }\n",
        "    }\n",
        "\n",
        "configs = get_augmentation_configs()\n",
        "print(\"Augmentation configurations:\")\n",
        "for variation, methods in configs.items():\n",
        "    print(f\"\\n{variation}:\")\n",
        "    for method, params in methods.items():\n",
        "        print(f\"  {method}: {params}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14",
      "metadata": {
        "id": "14"
      },
      "source": [
        "## Step 4: Training and Evaluation Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "15",
      "metadata": {
        "id": "15"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(clf, X_test, y_test):\n",
        "    \"\"\"\n",
        "    Evaluate model and return metrics for minority class (always class 1)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        y_pred = clf.predict(X_test)\n",
        "        y_pred_proba = clf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "        # Verify minority class is 1\n",
        "        class_counts = np.bincount(y_test)\n",
        "        minority_class = np.argmin(class_counts)\n",
        "        if minority_class != 1:\n",
        "            print(f\"Warning: Expected minority class to be 1, but found {minority_class}\")\n",
        "\n",
        "        metrics = {\n",
        "            'F1': round(f1_score(y_test, y_pred, pos_label=1), 4),\n",
        "            'Precision': round(precision_score(y_test, y_pred, pos_label=1), 4),\n",
        "            'Recall': round(recall_score(y_test, y_pred, pos_label=1), 4),\n",
        "            'AUC-ROC': round(roc_auc_score(y_test, y_pred_proba), 4)\n",
        "        }\n",
        "        return metrics\n",
        "    except Exception as e:\n",
        "        print(f\"Error in evaluation: {str(e)}\")\n",
        "        return {'F1': np.nan, 'Precision': np.nan, 'Recall': np.nan, 'AUC-ROC': np.nan}\n",
        "\n",
        "def run_experiment(df, target_col, dataset_name, ratio):\n",
        "    \"\"\"\n",
        "    Run complete experiment for one dataset and ratio\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"RUNNING EXPERIMENT: {dataset_name} - Ratio 1:{ratio}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # Prepare features and target\n",
        "    X = df.drop(target_col, axis=1).values\n",
        "    y = df[target_col].values\n",
        "\n",
        "    # Verify minority class is 1\n",
        "    class_counts = np.bincount(y)\n",
        "    minority_class = np.argmin(class_counts)\n",
        "    print(f\"Dataset class distribution: {dict(enumerate(class_counts))}\")\n",
        "    print(f\"Minority class: {minority_class} ({class_counts[minority_class]} samples)\")\n",
        "    print(f\"Majority class: {1-minority_class} ({class_counts[1-minority_class]} samples)\")\n",
        "\n",
        "    if minority_class != 1:\n",
        "        print(f\"ERROR: Expected minority class to be 1, but found {minority_class}\")\n",
        "        print(\"Please check dataset preprocessing!\")\n",
        "\n",
        "    # Train-test split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, stratify=y, random_state=42\n",
        "    )\n",
        "\n",
        "    # Standardize features\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    print(f\"Train shape: {X_train_scaled.shape}, Test shape: {X_test_scaled.shape}\")\n",
        "    print(f\"Train class distribution: {np.bincount(y_train)}\")\n",
        "    print(f\"Test class distribution: {np.bincount(y_test)}\")\n",
        "\n",
        "    # Initialize results storage\n",
        "    results = defaultdict(dict)\n",
        "    classifiers = get_classifiers()\n",
        "    augmentation_configs = get_augmentation_configs()\n",
        "\n",
        "    # Baseline (no augmentation) - compute once, use for both variations\n",
        "    print(\"\\n--- Running Baseline (No Augmentation) ---\")\n",
        "    baseline_results = {}\n",
        "    for clf_name, clf in classifiers.items():\n",
        "        print(f\"Training {clf_name}...\")\n",
        "        clf.fit(X_train_scaled, y_train)\n",
        "        metrics = evaluate_model(clf, X_test_scaled, y_test)\n",
        "        baseline_results[clf_name] = metrics\n",
        "        print(f\"  {clf_name} - F1: {metrics['F1']}, Precision: {metrics['Precision']}, Recall: {metrics['Recall']}, AUC: {metrics['AUC-ROC']}\")\n",
        "\n",
        "    # Store baseline for both variations\n",
        "    results['V1_Default']['Baseline'] = baseline_results.copy()\n",
        "    results['V2_Tuned']['Baseline'] = baseline_results.copy()\n",
        "\n",
        "    # Run augmentation methods for each variation\n",
        "    for variation_name, aug_config in augmentation_configs.items():\n",
        "        print(f\"\\n--- Running {variation_name} ---\")\n",
        "\n",
        "        for aug_method, aug_params in aug_config.items():\n",
        "            print(f\"\\nApplying {aug_method} augmentation...\")\n",
        "\n",
        "            # Apply augmentation\n",
        "            X_train_aug, y_train_aug = apply_augmentation(\n",
        "                X_train_scaled, y_train, aug_method, aug_params\n",
        "            )\n",
        "\n",
        "            print(f\"  Augmented training shape: {X_train_aug.shape}\")\n",
        "            print(f\"  Augmented class distribution: {np.bincount(y_train_aug)}\")\n",
        "\n",
        "            # Train classifiers on augmented data\n",
        "            method_results = {}\n",
        "            for clf_name, clf in classifiers.items():\n",
        "                print(f\"  Training {clf_name}...\")\n",
        "                try:\n",
        "                    clf.fit(X_train_aug, y_train_aug)\n",
        "                    metrics = evaluate_model(clf, X_test_scaled, y_test)\n",
        "                    method_results[clf_name] = metrics\n",
        "                    print(f\"    {clf_name} - F1: {metrics['F1']}, Precision: {metrics['Precision']}, Recall: {metrics['Recall']}, AUC: {metrics['AUC-ROC']}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"    Error training {clf_name}: {str(e)}\")\n",
        "                    method_results[clf_name] = {'F1': np.nan, 'Precision': np.nan, 'Recall': np.nan, 'AUC-ROC': np.nan}\n",
        "\n",
        "            results[variation_name][aug_method] = method_results\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "27cd7367",
      "metadata": {
        "id": "27cd7367"
      },
      "outputs": [],
      "source": [
        "def apply_augmentation(X_train, y_train, method, params):\n",
        "    \"\"\"\n",
        "    Apply specified augmentation method to training data\n",
        "    \"\"\"\n",
        "    if method == 'SMOTE':\n",
        "        smote = SMOTE(**params)\n",
        "        X_res, y_res = smote.fit_resample(X_train, y_train)\n",
        "    elif method == 'ADASYN':\n",
        "        adasyn = ADASYN(**params)\n",
        "        X_res, y_res = adasyn.fit_resample(X_train, y_train)\n",
        "    elif method == 'CTGAN':\n",
        "        # CTGAN requires dataframe and specific data types\n",
        "        # Convert numpy arrays back to DataFrame for CTGAN\n",
        "        # Assign string column names to prevent TypeError in rdt library\n",
        "        feature_cols = [f'feature_{i}' for i in range(X_train.shape[1])]\n",
        "        X_train_df = pd.DataFrame(X_train, columns=feature_cols)\n",
        "        y_train_df = pd.DataFrame(y_train, columns=['target'])\n",
        "        # Combine X and y for CTGAN training\n",
        "        data_for_ctgan = pd.concat([X_train_df, y_train_df], axis=1)\n",
        "\n",
        "        # Define categorical features for CTGAN (only target in this case)\n",
        "        categorical_features = ['target']\n",
        "        discrete_columns = [col for col in categorical_features if col in data_for_ctgan.columns]\n",
        "\n",
        "        # Ensure target column is treated as categorical by CTGAN\n",
        "        ctgan_model = CTGAN(epochs=params['epochs'], batch_size=params['batch_size'])\n",
        "        ctgan_model.fit(data_for_ctgan, discrete_columns=discrete_columns)\n",
        "\n",
        "        # Generate samples for the minority class\n",
        "        # Determine how many samples to generate to balance the dataset\n",
        "        minority_class = y_train_df['target'].value_counts().idxmin()\n",
        "        majority_class_count = y_train_df['target'].value_counts().max()\n",
        "        minority_class_count = y_train_df['target'].value_counts().min()\n",
        "\n",
        "        num_samples_to_generate = majority_class_count - minority_class_count\n",
        "\n",
        "        # Generate synthetic data with specific conditions if possible (e.g., for minority class)\n",
        "        # Note: CTGAN's generate method doesn't directly support generating only a specific class easily.\n",
        "        # A common approach is to oversample the original minority class within the CTGAN training data\n",
        "        # or filter generated data. For simplicity here, we'll generate and then filter/select.\n",
        "        # A more robust solution might involve conditional GANs or generating more data than needed and filtering.\n",
        "        synthetic_data = ctgan_model.sample(num_samples_to_generate)\n",
        "\n",
        "        # Filter synthetic data to primarily get minority class samples\n",
        "        # This is an approximation; ideally, CTGAN would be trained with a focus on minority class\n",
        "        synthetic_minority_samples = synthetic_data[synthetic_data['target'] == minority_class]\n",
        "\n",
        "        # If not enough minority samples generated, supplement with random samples from all generated\n",
        "        if len(synthetic_minority_samples) < num_samples_to_generate:\n",
        "            remaining_needed = num_samples_to_generate - len(synthetic_minority_samples)\n",
        "            # Take random samples from the rest of synthetic data until target count is met\n",
        "            # This might not be ideal as it could include majority class samples\n",
        "            synthetic_minority_samples = pd.concat([\n",
        "                synthetic_minority_samples,\n",
        "                synthetic_data[synthetic_data['target'] != minority_class].sample(n=remaining_needed, replace=True, random_state=42)\n",
        "            ]).reset_index(drop=True)\n",
        "\n",
        "        # Combine original data with synthetic minority samples\n",
        "        X_res_df = pd.concat([X_train_df, synthetic_minority_samples.drop(columns=['target'])], ignore_index=True)\n",
        "        y_res_df = pd.concat([y_train_df, synthetic_minority_samples[['target']]], ignore_index=True)\n",
        "\n",
        "        X_res = X_res_df.values\n",
        "        y_res = y_res_df['target'].values\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown augmentation method: {method}\")\n",
        "\n",
        "    return X_res, y_res"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16",
      "metadata": {
        "id": "16"
      },
      "source": [
        "## Step 5: Execute Experiments and Generate Comparison Tables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "17",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17",
        "outputId": "760e1d29-85cf-4485-9512-8f3b40f04afa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "RUNNING EXPERIMENT: Breast Cancer - Ratio 1:10\n",
            "============================================================\n",
            "Dataset class distribution: {0: np.int64(6160), 1: np.int64(616)}\n",
            "Minority class: 1 (616 samples)\n",
            "Majority class: 0 (6160 samples)\n",
            "Train shape: (5420, 29), Test shape: (1356, 29)\n",
            "Train class distribution: [4927  493]\n",
            "Test class distribution: [1233  123]\n",
            "\n",
            "--- Running Baseline (No Augmentation) ---\n",
            "Training LogisticRegression...\n",
            "  LogisticRegression - F1: 0.5169, Precision: 0.8364, Recall: 0.374, AUC: 0.8684\n",
            "Training RandomForest...\n",
            "  RandomForest - F1: 0.6023, Precision: 1.0, Recall: 0.4309, AUC: 0.9258\n",
            "Training XGBoost...\n",
            "  XGBoost - F1: 0.6489, Precision: 0.9385, Recall: 0.4959, AUC: 0.8642\n",
            "\n",
            "--- Running V1_Default ---\n",
            "\n",
            "Applying SMOTE augmentation...\n",
            "  Augmented training shape: (9854, 29)\n",
            "  Augmented class distribution: [4927 4927]\n",
            "  Training LogisticRegression...\n",
            "    LogisticRegression - F1: 0.4099, Precision: 0.2835, Recall: 0.7398, AUC: 0.8603\n",
            "  Training RandomForest...\n",
            "    RandomForest - F1: 0.6893, Precision: 0.8554, Recall: 0.5772, AUC: 0.9296\n",
            "  Training XGBoost...\n",
            "    XGBoost - F1: 0.6598, Precision: 0.9014, Recall: 0.5203, AUC: 0.8613\n",
            "\n",
            "Applying ADASYN augmentation...\n",
            "  Augmented training shape: (9803, 29)\n",
            "  Augmented class distribution: [4927 4876]\n",
            "  Training LogisticRegression...\n",
            "    LogisticRegression - F1: 0.3874, Precision: 0.2614, Recall: 0.748, AUC: 0.8578\n",
            "  Training RandomForest...\n",
            "    RandomForest - F1: 0.6699, Precision: 0.8313, Recall: 0.561, AUC: 0.9279\n",
            "  Training XGBoost...\n",
            "    XGBoost - F1: 0.6667, Precision: 0.9028, Recall: 0.5285, AUC: 0.8637\n",
            "\n",
            "Applying CTGAN augmentation...\n",
            "  Augmented training shape: (9854, 29)\n",
            "  Augmented class distribution: [7439 2415]\n",
            "  Training LogisticRegression...\n",
            "    LogisticRegression - F1: 0.4138, Precision: 0.525, Recall: 0.3415, AUC: 0.8461\n",
            "  Training RandomForest...\n",
            "    RandomForest - F1: 0.618, Precision: 1.0, Recall: 0.4472, AUC: 0.9159\n",
            "  Training XGBoost...\n",
            "    XGBoost - F1: 0.6304, Precision: 0.9508, Recall: 0.4715, AUC: 0.8712\n",
            "\n",
            "--- Running V2_Tuned ---\n",
            "\n",
            "Applying SMOTE augmentation...\n",
            "  Augmented training shape: (7390, 29)\n",
            "  Augmented class distribution: [4927 2463]\n",
            "  Training LogisticRegression...\n",
            "    LogisticRegression - F1: 0.5143, Precision: 0.4219, Recall: 0.6585, AUC: 0.8613\n",
            "  Training RandomForest...\n",
            "    RandomForest - F1: 0.6599, Precision: 0.8784, Recall: 0.5285, AUC: 0.9289\n",
            "  Training XGBoost...\n",
            "    XGBoost - F1: 0.6392, Precision: 0.8732, Recall: 0.5041, AUC: 0.8556\n",
            "\n",
            "Applying ADASYN augmentation...\n",
            "  Augmented training shape: (7465, 29)\n",
            "  Augmented class distribution: [4927 2538]\n",
            "  Training LogisticRegression...\n",
            "    LogisticRegression - F1: 0.4852, Precision: 0.3814, Recall: 0.6667, AUC: 0.8594\n",
            "  Training RandomForest...\n",
            "    RandomForest - F1: 0.6701, Precision: 0.8919, Recall: 0.5366, AUC: 0.9263\n",
            "  Training XGBoost...\n",
            "    XGBoost - F1: 0.6421, Precision: 0.9104, Recall: 0.4959, AUC: 0.8673\n",
            "\n",
            "Applying CTGAN augmentation...\n",
            "  Augmented training shape: (9854, 29)\n",
            "  Augmented class distribution: [7448 2406]\n",
            "  Training LogisticRegression...\n",
            "    LogisticRegression - F1: 0.4922, Precision: 0.4737, Recall: 0.5122, AUC: 0.845\n",
            "  Training RandomForest...\n",
            "    RandomForest - F1: 0.6188, Precision: 0.9655, Recall: 0.4553, AUC: 0.9117\n",
            "  Training XGBoost...\n",
            "    XGBoost - F1: 0.6486, Precision: 0.9677, Recall: 0.4878, AUC: 0.8756\n",
            "\n",
            "\n",
            "================================================================================\n",
            "COMPARISON TABLE: Breast Cancer - Ratio 1:10\n",
            "================================================================================\n",
            "Variation                   V1_Default                           V2_Tuned                          \n",
            "Metric                              F1 Precision  Recall AUC-ROC       F1 Precision  Recall AUC-ROC\n",
            "Method   Classifier                                                                                \n",
            "Baseline LogisticRegression     0.5169    0.8364   0.374  0.8684   0.5169    0.8364   0.374  0.8684\n",
            "         RandomForest           0.6023       1.0  0.4309  0.9258   0.6023       1.0  0.4309  0.9258\n",
            "         XGBoost                0.6489    0.9385  0.4959  0.8642   0.6489    0.9385  0.4959  0.8642\n",
            "SMOTE    LogisticRegression     0.4099    0.2835  0.7398  0.8603   0.5143    0.4219  0.6585  0.8613\n",
            "         RandomForest           0.6893    0.8554  0.5772  0.9296   0.6599    0.8784  0.5285  0.9289\n",
            "         XGBoost                0.6598    0.9014  0.5203  0.8613   0.6392    0.8732  0.5041  0.8556\n",
            "ADASYN   LogisticRegression     0.3874    0.2614   0.748  0.8578   0.4852    0.3814  0.6667  0.8594\n",
            "         RandomForest           0.6699    0.8313   0.561  0.9279   0.6701    0.8919  0.5366  0.9263\n",
            "         XGBoost                0.6667    0.9028  0.5285  0.8637   0.6421    0.9104  0.4959  0.8673\n",
            "CTGAN    LogisticRegression     0.4138     0.525  0.3415  0.8461   0.4922    0.4737  0.5122   0.845\n",
            "         RandomForest            0.618       1.0  0.4472  0.9159   0.6188    0.9655  0.4553  0.9117\n",
            "         XGBoost                0.6304    0.9508  0.4715  0.8712   0.6486    0.9677  0.4878  0.8756\n",
            "================================================================================\n",
            "\n",
            "============================================================\n",
            "RUNNING EXPERIMENT: Breast Cancer - Ratio 1:100\n",
            "============================================================\n",
            "Dataset class distribution: {0: np.int64(61600), 1: np.int64(616)}\n",
            "Minority class: 1 (616 samples)\n",
            "Majority class: 0 (61600 samples)\n",
            "Train shape: (49772, 29), Test shape: (12444, 29)\n",
            "Train class distribution: [49279   493]\n",
            "Test class distribution: [12321   123]\n",
            "\n",
            "--- Running Baseline (No Augmentation) ---\n",
            "Training LogisticRegression...\n",
            "  LogisticRegression - F1: 0.2553, Precision: 1.0, Recall: 0.1463, AUC: 0.8604\n",
            "Training RandomForest...\n",
            "  RandomForest - F1: 0.6023, Precision: 1.0, Recall: 0.4309, AUC: 0.9837\n",
            "Training XGBoost...\n",
            "  XGBoost - F1: 0.6023, Precision: 1.0, Recall: 0.4309, AUC: 0.8688\n",
            "\n",
            "--- Running V1_Default ---\n",
            "\n",
            "Applying SMOTE augmentation...\n",
            "  Augmented training shape: (98558, 29)\n",
            "  Augmented class distribution: [49279 49279]\n",
            "  Training LogisticRegression...\n",
            "    LogisticRegression - F1: 0.0729, Precision: 0.0383, Recall: 0.7398, AUC: 0.8587\n",
            "  Training RandomForest...\n",
            "    RandomForest - F1: 0.7513, Precision: 1.0, Recall: 0.6016, AUC: 1.0\n",
            "  Training XGBoost...\n",
            "    XGBoost - F1: 0.6484, Precision: 1.0, Recall: 0.4797, AUC: 0.8616\n",
            "\n",
            "Applying ADASYN augmentation...\n",
            "  Augmented training shape: (98693, 29)\n",
            "  Augmented class distribution: [49279 49414]\n",
            "  Training LogisticRegression...\n",
            "    LogisticRegression - F1: 0.0685, Precision: 0.0359, Recall: 0.748, AUC: 0.8588\n",
            "  Training RandomForest...\n",
            "    RandomForest - F1: 0.7513, Precision: 1.0, Recall: 0.6016, AUC: 0.9959\n",
            "  Training XGBoost...\n",
            "    XGBoost - F1: 0.6409, Precision: 1.0, Recall: 0.4715, AUC: 0.8706\n",
            "\n",
            "Applying CTGAN augmentation...\n",
            "  Augmented training shape: (98558, 29)\n",
            "  Augmented class distribution: [80410 18148]\n",
            "  Training LogisticRegression...\n",
            "    LogisticRegression - F1: 0.2179, Precision: 0.1488, Recall: 0.4065, AUC: 0.8437\n",
            "  Training RandomForest...\n",
            "    RandomForest - F1: 0.5943, Precision: 1.0, Recall: 0.4228, AUC: 0.9675\n",
            "  Training XGBoost...\n",
            "    XGBoost - F1: 0.472, Precision: 1.0, Recall: 0.3089, AUC: 0.8696\n",
            "\n",
            "--- Running V2_Tuned ---\n",
            "\n",
            "Applying SMOTE augmentation...\n",
            "  Augmented training shape: (73918, 29)\n",
            "  Augmented class distribution: [49279 24639]\n",
            "  Training LogisticRegression...\n",
            "    LogisticRegression - F1: 0.1249, Precision: 0.0691, Recall: 0.6504, AUC: 0.8602\n",
            "  Training RandomForest...\n",
            "    RandomForest - F1: 0.7513, Precision: 1.0, Recall: 0.6016, AUC: 0.9959\n",
            "  Training XGBoost...\n",
            "    XGBoost - F1: 0.6023, Precision: 1.0, Recall: 0.4309, AUC: 0.8761\n",
            "\n",
            "Applying ADASYN augmentation...\n",
            "  Augmented training shape: (73984, 29)\n",
            "  Augmented class distribution: [49279 24705]\n",
            "  Training LogisticRegression...\n",
            "    LogisticRegression - F1: 0.1183, Precision: 0.065, Recall: 0.6504, AUC: 0.8616\n",
            "  Training RandomForest...\n",
            "    RandomForest - F1: 0.7576, Precision: 1.0, Recall: 0.6098, AUC: 0.9959\n",
            "  Training XGBoost...\n",
            "    XGBoost - F1: 0.5862, Precision: 1.0, Recall: 0.4146, AUC: 0.8634\n",
            "\n",
            "Applying CTGAN augmentation...\n",
            "  Augmented training shape: (98558, 29)\n",
            "  Augmented class distribution: [80098 18460]\n",
            "  Training LogisticRegression...\n",
            "    LogisticRegression - F1: 0.2, Precision: 0.1302, Recall: 0.4309, AUC: 0.8446\n",
            "  Training RandomForest...\n",
            "    RandomForest - F1: 0.618, Precision: 1.0, Recall: 0.4472, AUC: 0.9675\n",
            "  Training XGBoost...\n",
            "    XGBoost - F1: 0.5, Precision: 1.0, Recall: 0.3333, AUC: 0.8773\n",
            "\n",
            "\n",
            "================================================================================\n",
            "COMPARISON TABLE: Breast Cancer - Ratio 1:100\n",
            "================================================================================\n",
            "Variation                   V1_Default                           V2_Tuned                          \n",
            "Metric                              F1 Precision  Recall AUC-ROC       F1 Precision  Recall AUC-ROC\n",
            "Method   Classifier                                                                                \n",
            "Baseline LogisticRegression     0.2553       1.0  0.1463  0.8604   0.2553       1.0  0.1463  0.8604\n",
            "         RandomForest           0.6023       1.0  0.4309  0.9837   0.6023       1.0  0.4309  0.9837\n",
            "         XGBoost                0.6023       1.0  0.4309  0.8688   0.6023       1.0  0.4309  0.8688\n",
            "SMOTE    LogisticRegression     0.0729    0.0383  0.7398  0.8587   0.1249    0.0691  0.6504  0.8602\n",
            "         RandomForest           0.7513       1.0  0.6016     1.0   0.7513       1.0  0.6016  0.9959\n",
            "         XGBoost                0.6484       1.0  0.4797  0.8616   0.6023       1.0  0.4309  0.8761\n",
            "ADASYN   LogisticRegression     0.0685    0.0359   0.748  0.8588   0.1183     0.065  0.6504  0.8616\n",
            "         RandomForest           0.7513       1.0  0.6016  0.9959   0.7576       1.0  0.6098  0.9959\n",
            "         XGBoost                0.6409       1.0  0.4715  0.8706   0.5862       1.0  0.4146  0.8634\n",
            "CTGAN    LogisticRegression     0.2179    0.1488  0.4065  0.8437      0.2    0.1302  0.4309  0.8446\n",
            "         RandomForest           0.5943       1.0  0.4228  0.9675    0.618       1.0  0.4472  0.9675\n",
            "         XGBoost                 0.472       1.0  0.3089  0.8696      0.5       1.0  0.3333  0.8773\n",
            "================================================================================\n",
            "\n",
            "============================================================\n",
            "RUNNING EXPERIMENT: Credit Card - Ratio 1:10\n",
            "============================================================\n",
            "Dataset class distribution: {0: np.int64(4920), 1: np.int64(492)}\n",
            "Minority class: 1 (492 samples)\n",
            "Majority class: 0 (4920 samples)\n",
            "Train shape: (4329, 30), Test shape: (1083, 30)\n",
            "Train class distribution: [3935  394]\n",
            "Test class distribution: [985  98]\n",
            "\n",
            "--- Running Baseline (No Augmentation) ---\n",
            "Training LogisticRegression...\n",
            "  LogisticRegression - F1: 0.9297, Precision: 0.9885, Recall: 0.8776, AUC: 0.9927\n",
            "Training RandomForest...\n",
            "  RandomForest - F1: 0.929, Precision: 1.0, Recall: 0.8673, AUC: 0.9935\n",
            "Training XGBoost...\n",
            "  XGBoost - F1: 0.9405, Precision: 1.0, Recall: 0.8878, AUC: 0.9968\n",
            "\n",
            "--- Running V1_Default ---\n",
            "\n",
            "Applying SMOTE augmentation...\n",
            "  Augmented training shape: (7870, 30)\n",
            "  Augmented class distribution: [3935 3935]\n",
            "  Training LogisticRegression...\n",
            "    LogisticRegression - F1: 0.8692, Precision: 0.8017, Recall: 0.949, AUC: 0.9912\n",
            "  Training RandomForest...\n",
            "    RandomForest - F1: 0.9255, Precision: 0.9667, Recall: 0.8878, AUC: 0.9957\n",
            "  Training XGBoost...\n",
            "    XGBoost - F1: 0.9146, Precision: 0.901, Recall: 0.9286, AUC: 0.9971\n",
            "\n",
            "Applying ADASYN augmentation...\n",
            "  Augmented training shape: (7862, 30)\n",
            "  Augmented class distribution: [3935 3927]\n",
            "  Training LogisticRegression...\n",
            "    LogisticRegression - F1: 0.6552, Precision: 0.4948, Recall: 0.9694, AUC: 0.9921\n",
            "  Training RandomForest...\n",
            "    RandomForest - F1: 0.9215, Precision: 0.9462, Recall: 0.898, AUC: 0.9964\n",
            "  Training XGBoost...\n",
            "    XGBoost - F1: 0.9286, Precision: 0.9286, Recall: 0.9286, AUC: 0.9971\n",
            "\n",
            "Applying CTGAN augmentation...\n",
            "  Augmented training shape: (7870, 30)\n",
            "  Augmented class distribution: [5972 1898]\n",
            "  Training LogisticRegression...\n",
            "    LogisticRegression - F1: 0.9239, Precision: 0.9884, Recall: 0.8673, AUC: 0.9921\n",
            "  Training RandomForest...\n",
            "    RandomForest - F1: 0.929, Precision: 1.0, Recall: 0.8673, AUC: 0.9958\n",
            "  Training XGBoost...\n",
            "    XGBoost - F1: 0.9305, Precision: 0.9775, Recall: 0.8878, AUC: 0.9972\n",
            "\n",
            "--- Running V2_Tuned ---\n",
            "\n",
            "Applying SMOTE augmentation...\n",
            "  Augmented training shape: (5902, 30)\n",
            "  Augmented class distribution: [3935 1967]\n",
            "  Training LogisticRegression...\n",
            "    LogisticRegression - F1: 0.901, Precision: 0.875, Recall: 0.9286, AUC: 0.992\n",
            "  Training RandomForest...\n",
            "    RandomForest - F1: 0.9355, Precision: 0.9886, Recall: 0.8878, AUC: 0.9971\n",
            "  Training XGBoost...\n",
            "    XGBoost - F1: 0.9326, Precision: 0.9474, Recall: 0.9184, AUC: 0.9975\n",
            "\n",
            "Applying ADASYN augmentation...\n",
            "  Augmented training shape: (5933, 30)\n",
            "  Augmented class distribution: [3935 1998]\n",
            "  Training LogisticRegression...\n",
            "    LogisticRegression - F1: 0.8139, Precision: 0.7068, Recall: 0.9592, AUC: 0.9911\n",
            "  Training RandomForest...\n",
            "    RandomForest - F1: 0.9016, Precision: 0.9158, Recall: 0.8878, AUC: 0.9966\n",
            "  Training XGBoost...\n",
            "    XGBoost - F1: 0.9184, Precision: 0.9184, Recall: 0.9184, AUC: 0.9954\n",
            "\n",
            "Applying CTGAN augmentation...\n",
            "  Augmented training shape: (7870, 30)\n",
            "  Augmented class distribution: [5973 1897]\n",
            "  Training LogisticRegression...\n",
            "    LogisticRegression - F1: 0.8973, Precision: 0.954, Recall: 0.8469, AUC: 0.9631\n",
            "  Training RandomForest...\n",
            "    RandomForest - F1: 0.929, Precision: 1.0, Recall: 0.8673, AUC: 0.9912\n",
            "  Training XGBoost...\n",
            "    XGBoost - F1: 0.9355, Precision: 0.9886, Recall: 0.8878, AUC: 0.9959\n",
            "\n",
            "\n",
            "================================================================================\n",
            "COMPARISON TABLE: Credit Card - Ratio 1:10\n",
            "================================================================================\n",
            "Variation                   V1_Default                           V2_Tuned                          \n",
            "Metric                              F1 Precision  Recall AUC-ROC       F1 Precision  Recall AUC-ROC\n",
            "Method   Classifier                                                                                \n",
            "Baseline LogisticRegression     0.9297    0.9885  0.8776  0.9927   0.9297    0.9885  0.8776  0.9927\n",
            "         RandomForest            0.929       1.0  0.8673  0.9935    0.929       1.0  0.8673  0.9935\n",
            "         XGBoost                0.9405       1.0  0.8878  0.9968   0.9405       1.0  0.8878  0.9968\n",
            "SMOTE    LogisticRegression     0.8692    0.8017   0.949  0.9912    0.901     0.875  0.9286   0.992\n",
            "         RandomForest           0.9255    0.9667  0.8878  0.9957   0.9355    0.9886  0.8878  0.9971\n",
            "         XGBoost                0.9146     0.901  0.9286  0.9971   0.9326    0.9474  0.9184  0.9975\n",
            "ADASYN   LogisticRegression     0.6552    0.4948  0.9694  0.9921   0.8139    0.7068  0.9592  0.9911\n",
            "         RandomForest           0.9215    0.9462   0.898  0.9964   0.9016    0.9158  0.8878  0.9966\n",
            "         XGBoost                0.9286    0.9286  0.9286  0.9971   0.9184    0.9184  0.9184  0.9954\n",
            "CTGAN    LogisticRegression     0.9239    0.9884  0.8673  0.9921   0.8973     0.954  0.8469  0.9631\n",
            "         RandomForest            0.929       1.0  0.8673  0.9958    0.929       1.0  0.8673  0.9912\n",
            "         XGBoost                0.9305    0.9775  0.8878  0.9972   0.9355    0.9886  0.8878  0.9959\n",
            "================================================================================\n",
            "\n",
            "============================================================\n",
            "RUNNING EXPERIMENT: Credit Card - Ratio 1:100\n",
            "============================================================\n",
            "Dataset class distribution: {0: np.int64(49200), 1: np.int64(492)}\n",
            "Minority class: 1 (492 samples)\n",
            "Majority class: 0 (49200 samples)\n",
            "Train shape: (39753, 30), Test shape: (9939, 30)\n",
            "Train class distribution: [39359   394]\n",
            "Test class distribution: [9841   98]\n",
            "\n",
            "--- Running Baseline (No Augmentation) ---\n",
            "Training LogisticRegression...\n",
            "  LogisticRegression - F1: 0.9081, Precision: 0.9655, Recall: 0.8571, AUC: 0.9744\n",
            "Training RandomForest...\n",
            "  RandomForest - F1: 0.9189, Precision: 0.977, Recall: 0.8673, AUC: 0.9541\n",
            "Training XGBoost...\n",
            "  XGBoost - F1: 0.9043, Precision: 0.9444, Recall: 0.8673, AUC: 0.9746\n",
            "\n",
            "--- Running V1_Default ---\n",
            "\n",
            "Applying SMOTE augmentation...\n",
            "  Augmented training shape: (78718, 30)\n",
            "  Augmented class distribution: [39359 39359]\n",
            "  Training LogisticRegression...\n",
            "    LogisticRegression - F1: 0.4018, Precision: 0.2571, Recall: 0.9184, AUC: 0.9717\n",
            "  Training RandomForest...\n",
            "    RandomForest - F1: 0.8854, Precision: 0.9043, Recall: 0.8673, AUC: 0.9789\n",
            "  Training XGBoost...\n",
            "    XGBoost - F1: 0.8788, Precision: 0.87, Recall: 0.8878, AUC: 0.9788\n",
            "\n",
            "Applying ADASYN augmentation...\n",
            "  Augmented training shape: (78709, 30)\n",
            "  Augmented class distribution: [39359 39350]\n",
            "  Training LogisticRegression...\n",
            "    LogisticRegression - F1: 0.1581, Precision: 0.0863, Recall: 0.9388, AUC: 0.9621\n",
            "  Training RandomForest...\n",
            "    RandomForest - F1: 0.8995, Precision: 0.9341, Recall: 0.8673, AUC: 0.9817\n",
            "  Training XGBoost...\n",
            "    XGBoost - F1: 0.839, Precision: 0.8037, Recall: 0.8776, AUC: 0.9756\n",
            "\n",
            "Applying CTGAN augmentation...\n",
            "  Augmented training shape: (78718, 30)\n",
            "  Augmented class distribution: [64237 14481]\n",
            "  Training LogisticRegression...\n",
            "    LogisticRegression - F1: 0.8447, Precision: 0.8056, Recall: 0.8878, AUC: 0.9661\n",
            "  Training RandomForest...\n",
            "    RandomForest - F1: 0.8821, Precision: 0.8866, Recall: 0.8776, AUC: 0.9546\n",
            "  Training XGBoost...\n",
            "    XGBoost - F1: 0.9005, Precision: 0.9247, Recall: 0.8776, AUC: 0.968\n",
            "\n",
            "--- Running V2_Tuned ---\n",
            "\n",
            "Applying SMOTE augmentation...\n",
            "  Augmented training shape: (59038, 30)\n",
            "  Augmented class distribution: [39359 19679]\n",
            "  Training LogisticRegression...\n",
            "    LogisticRegression - F1: 0.5687, Precision: 0.414, Recall: 0.9082, AUC: 0.9726\n",
            "  Training RandomForest...\n",
            "    RandomForest - F1: 0.9101, Precision: 0.9451, Recall: 0.8776, AUC: 0.9774\n",
            "  Training XGBoost...\n",
            "    XGBoost - F1: 0.911, Precision: 0.9355, Recall: 0.8878, AUC: 0.9787\n",
            "\n",
            "Applying ADASYN augmentation...\n",
            "  Augmented training shape: (59059, 30)\n",
            "  Augmented class distribution: [39359 19700]\n",
            "  Training LogisticRegression...\n",
            "    LogisticRegression - F1: 0.2835, Precision: 0.1673, Recall: 0.9286, AUC: 0.9615\n",
            "  Training RandomForest...\n",
            "    RandomForest - F1: 0.8901, Precision: 0.914, Recall: 0.8673, AUC: 0.9842\n",
            "  Training XGBoost...\n",
            "    XGBoost - F1: 0.8447, Precision: 0.8056, Recall: 0.8878, AUC: 0.9764\n",
            "\n",
            "Applying CTGAN augmentation...\n",
            "  Augmented training shape: (78718, 30)\n",
            "  Augmented class distribution: [64239 14479]\n",
            "  Training LogisticRegression...\n",
            "    LogisticRegression - F1: 0.8286, Precision: 0.7768, Recall: 0.8878, AUC: 0.966\n",
            "  Training RandomForest...\n",
            "    RandomForest - F1: 0.8958, Precision: 0.9149, Recall: 0.8776, AUC: 0.9654\n",
            "  Training XGBoost...\n",
            "    XGBoost - F1: 0.87, Precision: 0.8529, Recall: 0.8878, AUC: 0.9726\n",
            "\n",
            "\n",
            "================================================================================\n",
            "COMPARISON TABLE: Credit Card - Ratio 1:100\n",
            "================================================================================\n",
            "Variation                   V1_Default                           V2_Tuned                          \n",
            "Metric                              F1 Precision  Recall AUC-ROC       F1 Precision  Recall AUC-ROC\n",
            "Method   Classifier                                                                                \n",
            "Baseline LogisticRegression     0.9081    0.9655  0.8571  0.9744   0.9081    0.9655  0.8571  0.9744\n",
            "         RandomForest           0.9189     0.977  0.8673  0.9541   0.9189     0.977  0.8673  0.9541\n",
            "         XGBoost                0.9043    0.9444  0.8673  0.9746   0.9043    0.9444  0.8673  0.9746\n",
            "SMOTE    LogisticRegression     0.4018    0.2571  0.9184  0.9717   0.5687     0.414  0.9082  0.9726\n",
            "         RandomForest           0.8854    0.9043  0.8673  0.9789   0.9101    0.9451  0.8776  0.9774\n",
            "         XGBoost                0.8788      0.87  0.8878  0.9788    0.911    0.9355  0.8878  0.9787\n",
            "ADASYN   LogisticRegression     0.1581    0.0863  0.9388  0.9621   0.2835    0.1673  0.9286  0.9615\n",
            "         RandomForest           0.8995    0.9341  0.8673  0.9817   0.8901     0.914  0.8673  0.9842\n",
            "         XGBoost                 0.839    0.8037  0.8776  0.9756   0.8447    0.8056  0.8878  0.9764\n",
            "CTGAN    LogisticRegression     0.8447    0.8056  0.8878  0.9661   0.8286    0.7768  0.8878   0.966\n",
            "         RandomForest           0.8821    0.8866  0.8776  0.9546   0.8958    0.9149  0.8776  0.9654\n",
            "         XGBoost                0.9005    0.9247  0.8776   0.968     0.87    0.8529  0.8878  0.9726\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "def create_comparison_table(results, dataset_name, ratio):\n",
        "    \"\"\"\n",
        "    Create comparison table combining V1 and V2 results for one ratio\n",
        "    \"\"\"\n",
        "    methods = ['Baseline', 'SMOTE', 'ADASYN', 'CTGAN']\n",
        "    classifiers = ['LogisticRegression', 'RandomForest', 'XGBoost']\n",
        "    metrics = ['F1', 'Precision', 'Recall', 'AUC-ROC']\n",
        "\n",
        "    # Create multi-index columns\n",
        "    columns = []\n",
        "    for variation in ['V1_Default', 'V2_Tuned']:\n",
        "        for metric in metrics:\n",
        "            columns.append((variation, metric))\n",
        "\n",
        "    multi_index = pd.MultiIndex.from_tuples(columns, names=['Variation', 'Metric'])\n",
        "\n",
        "    # Create row index\n",
        "    row_index = []\n",
        "    for method in methods:\n",
        "        for clf in classifiers:\n",
        "            row_index.append((method, clf))\n",
        "\n",
        "    multi_row_index = pd.MultiIndex.from_tuples(row_index, names=['Method', 'Classifier'])\n",
        "\n",
        "    # Create DataFrame\n",
        "    comparison_df = pd.DataFrame(index=multi_row_index, columns=multi_index)\n",
        "\n",
        "    # Fill data\n",
        "    for variation in ['V1_Default', 'V2_Tuned']:\n",
        "        for method in methods:\n",
        "            for clf in classifiers:\n",
        "                for metric in metrics:\n",
        "                    try:\n",
        "                        value = results[variation][method][clf][metric]\n",
        "                        comparison_df.loc[(method, clf), (variation, metric)] = value\n",
        "                    except KeyError:\n",
        "                        comparison_df.loc[(method, clf), (variation, metric)] = np.nan\n",
        "\n",
        "    return comparison_df\n",
        "\n",
        "# Run all experiments\n",
        "all_results = {}\n",
        "datasets_info = [\n",
        "    ('breast_cancer', 'Status', 'Breast Cancer'),\n",
        "    ('creditcard', 'Class', 'Credit Card')\n",
        "]\n",
        "\n",
        "for dataset_prefix, target_col, dataset_display_name in datasets_info:\n",
        "    all_results[dataset_prefix] = {}\n",
        "\n",
        "    for ratio in [10, 100]:\n",
        "        # Load imbalanced dataset\n",
        "        filename = f\"imbalanced_datasets/{dataset_prefix}_1_{ratio}.csv\"\n",
        "        df = pd.read_csv(filename)\n",
        "\n",
        "        # Ensure target column is numerical (0 and 1) and encode categorical features\n",
        "        if dataset_prefix == 'breast_cancer':\n",
        "            df[target_col] = df[target_col].map({'Alive': 0, 'Dead': 1}).astype(int)\n",
        "            # Dynamically identify categorical features (object dtype)\n",
        "            categorical_cols_to_encode = df.select_dtypes(include='object').columns.tolist()\n",
        "            # Ensure target_col is not in the list to be encoded if it somehow got there\n",
        "            if target_col in categorical_cols_to_encode:\n",
        "                categorical_cols_to_encode.remove(target_col)\n",
        "            df = pd.get_dummies(df, columns=categorical_cols_to_encode, drop_first=True)\n",
        "        elif dataset_prefix == 'creditcard':\n",
        "            df[target_col] = df[target_col].astype(int) # Ensure it's int just in case\n",
        "            # Credit card data is already numerical, no additional feature encoding needed.\n",
        "\n",
        "        # Run experiment\n",
        "        results = run_experiment(df, target_col, dataset_display_name, ratio)\n",
        "        all_results[dataset_prefix][f\"1_{ratio}\"] = results\n",
        "\n",
        "        # Create and display comparison table\n",
        "        comparison_table = create_comparison_table(results, dataset_display_name, ratio)\n",
        "\n",
        "        print(f\"\\n\\n{'='*80}\")\n",
        "        print(f\"COMPARISON TABLE: {dataset_display_name} - Ratio 1:{ratio}\")\n",
        "        print(f\"{'='*80}\")\n",
        "        print(comparison_table.to_string())\n",
        "        print(f\"{'='*80}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18",
      "metadata": {
        "id": "18"
      },
      "source": [
        "## Step 6: Save Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "19",
      "metadata": {
        "id": "19",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fd36e91-51ec-4b93-cc01-46871f86f90b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "SAVING RESULTS\n",
            "============================================================\n",
            "Saved: results/breast_cancer_results.xlsx\n",
            "Saved: results/creditcard_results.xlsx\n",
            "Saved: results/all_results_summary.xlsx\n",
            "Saved: experiment_config.json\n",
            "\n",
            "All results saved successfully!\n"
          ]
        }
      ],
      "source": [
        "def save_results_to_excel(all_results):\n",
        "    \"\"\"\n",
        "    Save results to Excel files with multiple sheets\n",
        "    \"\"\"\n",
        "    # Create the 'results' directory if it doesn't exist\n",
        "    os.makedirs('results', exist_ok=True)\n",
        "\n",
        "    # Save individual dataset results\n",
        "    for dataset_name, dataset_results in all_results.items():\n",
        "        filename = f\"results/{dataset_name}_results.xlsx\"\n",
        "\n",
        "        with pd.ExcelWriter(filename, engine='openpyxl') as writer:\n",
        "            for ratio_name, results in dataset_results.items():\n",
        "                ratio_num = ratio_name.split('_')[1]\n",
        "                dataset_display = 'Breast Cancer' if dataset_name == 'breast_cancer' else 'Credit Card'\n",
        "\n",
        "                comparison_table = create_comparison_table(results, dataset_display, int(ratio_num))\n",
        "                comparison_table.to_excel(writer, sheet_name=f\"Ratio_1_{ratio_num}\")\n",
        "\n",
        "        print(f\"Saved: {filename}\")\n",
        "\n",
        "    # Save combined results\n",
        "    with pd.ExcelWriter('results/all_results_summary.xlsx', engine='openpyxl') as writer:\n",
        "        for dataset_name, dataset_results in all_results.items():\n",
        "            for ratio_name, results in dataset_results.items():\n",
        "                ratio_num = ratio_name.split('_')[1]\n",
        "                dataset_display = 'Breast Cancer' if dataset_name == 'breast_cancer' else 'Credit Card'\n",
        "\n",
        "                comparison_table = create_comparison_table(results, dataset_display, int(ratio_num))\n",
        "                sheet_name = f\"{dataset_name}_1_{ratio_num}\"\n",
        "                comparison_table.to_excel(writer, sheet_name=sheet_name)\n",
        "\n",
        "    print(\"Saved: results/all_results_summary.xlsx\")\n",
        "\n",
        "def save_experiment_config():\n",
        "    \"\"\"\n",
        "    Save experiment configuration to JSON\n",
        "    \"\"\"\n",
        "    config = {\n",
        "        'datasets': ['breast_cancer', 'creditcard'],\n",
        "        'imbalance_ratios': [10, 100],\n",
        "        'classifiers': {\n",
        "            'LogisticRegression': {'max_iter': 1000, 'random_state': 42},\n",
        "            'RandomForest': {'n_estimators': 100, 'random_state': 42},\n",
        "            'XGBoost': {'n_estimators': 100, 'eval_metric': 'logloss', 'random_state': 42, 'use_label_encoder': False}\n",
        "        },\n",
        "        'augmentation_configs': get_augmentation_configs(),\n",
        "        'train_test_split': {'test_size': 0.2, 'stratify': True, 'random_state': 42},\n",
        "        'metrics': ['F1', 'Precision', 'Recall', 'AUC-ROC'],\n",
        "        'pos_label': 1\n",
        "    }\n",
        "\n",
        "    with open('experiment_config.json', 'w') as f:\n",
        "        json.dump(config, f, indent=2)\n",
        "\n",
        "    print(\"Saved: experiment_config.json\")\n",
        "\n",
        "# Save all results\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"SAVING RESULTS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "save_results_to_excel(all_results)\n",
        "save_experiment_config()\n",
        "\n",
        "print(\"\\nAll results saved successfully!\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}